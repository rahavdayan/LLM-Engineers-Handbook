[
  {
    "start": "00:00:03.960000",
    "end": "00:00:07.550000",
    "text": "in this Example The Notebook uh is quite\nuh instructive because it refers to a"
  },
  {
    "start": "00:00:07.560000",
    "end": "00:00:10.709000",
    "text": "uh instructive because it refers to a\nsmall data set um and I think uh working"
  },
  {
    "start": "00:00:10.719000",
    "end": "00:00:12.870000",
    "text": "small data set um and I think uh working\nwith small data sets are actually handy"
  },
  {
    "start": "00:00:12.880000",
    "end": "00:00:15.190000",
    "text": "with small data sets are actually handy\nin the beginning when you are trying to"
  },
  {
    "start": "00:00:15.200000",
    "end": "00:00:17.710000",
    "text": "in the beginning when you are trying to\nunderstand what is going on here we have"
  },
  {
    "start": "00:00:17.720000",
    "end": "00:00:21.310000",
    "text": "understand what is going on here we have\nthe classic case of dogs versus cats we"
  },
  {
    "start": "00:00:21.320000",
    "end": "00:00:24.029000",
    "text": "the classic case of dogs versus cats we\nhave also the simplest possible task in"
  },
  {
    "start": "00:00:24.039000",
    "end": "00:00:26.390000",
    "text": "have also the simplest possible task in\nmachine learning which is classification"
  },
  {
    "start": "00:00:26.400000",
    "end": "00:00:29.269000",
    "text": "machine learning which is classification\nimage classification in this case and we"
  },
  {
    "start": "00:00:29.279000",
    "end": "00:00:30.870000",
    "text": "image classification in this case and we\nare going to be using convolutional"
  },
  {
    "start": "00:00:30.880000",
    "end": "00:00:34.110000",
    "text": "are going to be using convolutional\nlayers in order to detect no no sorry to"
  },
  {
    "start": "00:00:34.120000",
    "end": "00:00:35.389000",
    "text": "layers in order to detect no no sorry to\ndetect to"
  },
  {
    "start": "00:00:35.399000",
    "end": "00:00:39.430000",
    "text": "detect to\nclassify uh the um presence of a dog or"
  },
  {
    "start": "00:00:39.440000",
    "end": "00:00:42.910000",
    "text": "classify uh the um presence of a dog or\na cat on in in an image okay or cats in"
  },
  {
    "start": "00:00:42.920000",
    "end": "00:00:46.869000",
    "text": "a cat on in in an image okay or cats in\nthis case all right so um the uh uh data"
  },
  {
    "start": "00:00:46.879000",
    "end": "00:00:49.189000",
    "text": "this case all right so um the uh uh data\nset is available in kagle the original"
  },
  {
    "start": "00:00:49.199000",
    "end": "00:00:52.310000",
    "text": "set is available in kagle the original\ndata set contained 25,000 images but we"
  },
  {
    "start": "00:00:52.320000",
    "end": "00:00:56.670000",
    "text": "data set contained 25,000 images but we\nhave cut down uh to 1,000 images uh per"
  },
  {
    "start": "00:00:56.680000",
    "end": "00:01:00.389000",
    "text": "have cut down uh to 1,000 images uh per\nclass and uh we have a split the data"
  },
  {
    "start": "00:01:00.399000",
    "end": "00:01:03.750000",
    "text": "class and uh we have a split the data\nset into uh train and validation and"
  },
  {
    "start": "00:01:03.760000",
    "end": "00:01:07.429000",
    "text": "set into uh train and validation and\ntest data set okay all right so we are"
  },
  {
    "start": "00:01:07.439000",
    "end": "00:01:09.990000",
    "text": "test data set okay all right so we are\ngoing to uh obviously use train and"
  },
  {
    "start": "00:01:10.000000",
    "end": "00:01:11.990000",
    "text": "going to uh obviously use train and\nvalidation to create if you like our"
  },
  {
    "start": "00:01:12.000000",
    "end": "00:01:14.350000",
    "text": "validation to create if you like our\nmodel and of course we are going to"
  },
  {
    "start": "00:01:14.360000",
    "end": "00:01:18.070000",
    "text": "model and of course we are going to\nexercise some kind of prediction API uh"
  },
  {
    "start": "00:01:18.080000",
    "end": "00:01:20.510000",
    "text": "exercise some kind of prediction API uh\nusing our test data set after a model is"
  },
  {
    "start": "00:01:20.520000",
    "end": "00:01:22.749000",
    "text": "using our test data set after a model is\ncreated so the architecture we're going"
  },
  {
    "start": "00:01:22.759000",
    "end": "00:01:25.670000",
    "text": "created so the architecture we're going\nto be using here is an architecture that"
  },
  {
    "start": "00:01:25.680000",
    "end": "00:01:27.910000",
    "text": "to be using here is an architecture that\nwe have kind of developed specifically"
  },
  {
    "start": "00:01:27.920000",
    "end": "00:01:30.030000",
    "text": "we have kind of developed specifically\nfor this example"
  },
  {
    "start": "00:01:30.040000",
    "end": "00:01:33.910000",
    "text": "for this example\nis uh consist evidently of convolutional"
  },
  {
    "start": "00:01:33.920000",
    "end": "00:01:36.069000",
    "text": "is uh consist evidently of convolutional\nand and interleaf with Max pulling"
  },
  {
    "start": "00:01:36.079000",
    "end": "00:01:39.149000",
    "text": "and and interleaf with Max pulling\nlayers and uh probably you recognize uh"
  },
  {
    "start": "00:01:39.159000",
    "end": "00:01:42.230000",
    "text": "layers and uh probably you recognize uh\nthe API here in this case is a kind of a"
  },
  {
    "start": "00:01:42.240000",
    "end": "00:01:45.630000",
    "text": "the API here in this case is a kind of a\ncaras API uh similar architectures can"
  },
  {
    "start": "00:01:45.640000",
    "end": "00:01:49.429000",
    "text": "caras API uh similar architectures can\nbe develop for py the um first layer"
  },
  {
    "start": "00:01:49.439000",
    "end": "00:01:52.149000",
    "text": "be develop for py the um first layer\nover here is a convolutional layer uh"
  },
  {
    "start": "00:01:52.159000",
    "end": "00:01:57.109000",
    "text": "over here is a convolutional layer uh\nthe um uh there is um uh input images of"
  },
  {
    "start": "00:01:57.119000",
    "end": "00:02:00.029000",
    "text": "the um uh there is um uh input images of\n150 by 150 pixels this is what the imag"
  },
  {
    "start": "00:02:00.039000",
    "end": "00:02:03.630000",
    "text": "150 by 150 pixels this is what the imag\nthat we have uh transformed uh now are"
  },
  {
    "start": "00:02:03.640000",
    "end": "00:02:05.749000",
    "text": "that we have uh transformed uh now are\nand each image is a naturally colored"
  },
  {
    "start": "00:02:05.759000",
    "end": "00:02:08.869000",
    "text": "and each image is a naturally colored\nimage of three channels red green and"
  },
  {
    "start": "00:02:08.879000",
    "end": "00:02:14.190000",
    "text": "image of three channels red green and\nblue we have U 3x3 kernels and uh we"
  },
  {
    "start": "00:02:14.200000",
    "end": "00:02:16.630000",
    "text": "blue we have U 3x3 kernels and uh we\nhave the 32 here indicates the number of"
  },
  {
    "start": "00:02:16.640000",
    "end": "00:02:19.830000",
    "text": "have the 32 here indicates the number of\nfilters okay or convolutional neurons"
  },
  {
    "start": "00:02:19.840000",
    "end": "00:02:22.150000",
    "text": "filters okay or convolutional neurons\nand we are going to be using a rectified"
  },
  {
    "start": "00:02:22.160000",
    "end": "00:02:24.750000",
    "text": "and we are going to be using a rectified\nlinear unit they exactly the same"
  },
  {
    "start": "00:02:24.760000",
    "end": "00:02:27.390000",
    "text": "linear unit they exactly the same\nnonlinearity that we have used uh in the"
  },
  {
    "start": "00:02:27.400000",
    "end": "00:02:28.670000",
    "text": "nonlinearity that we have used uh in the\nfully connected"
  },
  {
    "start": "00:02:28.680000",
    "end": "00:02:32.309000",
    "text": "fully connected\nlayers uh then we are passing the output"
  },
  {
    "start": "00:02:32.319000",
    "end": "00:02:34.990000",
    "text": "layers uh then we are passing the output\nfeature map produced here and by the way"
  },
  {
    "start": "00:02:35.000000",
    "end": "00:02:37.430000",
    "text": "feature map produced here and by the way\nthis is where you can actually see the"
  },
  {
    "start": "00:02:37.440000",
    "end": "00:02:40.430000",
    "text": "this is where you can actually see the\num uh the usage of that kind of formula"
  },
  {
    "start": "00:02:40.440000",
    "end": "00:02:41.949000",
    "text": "um uh the usage of that kind of formula\nwhich I was pointing out regarding the"
  },
  {
    "start": "00:02:41.959000",
    "end": "00:02:44.110000",
    "text": "which I was pointing out regarding the\noutput feature map dimensions in an"
  },
  {
    "start": "00:02:44.120000",
    "end": "00:02:47.350000",
    "text": "output feature map dimensions in an\nearlier video uh the uh Max pooling"
  },
  {
    "start": "00:02:47.360000",
    "end": "00:02:50.470000",
    "text": "earlier video uh the uh Max pooling\nlayer in this case is 2x two and it will"
  },
  {
    "start": "00:02:50.480000",
    "end": "00:02:53.949000",
    "text": "layer in this case is 2x two and it will\nfurther uh shrink uh the output feat M"
  },
  {
    "start": "00:02:53.959000",
    "end": "00:02:56.670000",
    "text": "further uh shrink uh the output feat M\nproduced by the first layer selecting"
  },
  {
    "start": "00:02:56.680000",
    "end": "00:02:58.990000",
    "text": "produced by the first layer selecting\nthe most important features out of it"
  },
  {
    "start": "00:02:59.000000",
    "end": "00:03:00.869000",
    "text": "the most important features out of it\npassing it over to to a convolutional"
  },
  {
    "start": "00:03:00.879000",
    "end": "00:03:04.390000",
    "text": "passing it over to to a convolutional\nlayer uh with uh 64 filters here you see"
  },
  {
    "start": "00:03:04.400000",
    "end": "00:03:06.430000",
    "text": "layer uh with uh 64 filters here you see\nnow the pattern of increasing the number"
  },
  {
    "start": "00:03:06.440000",
    "end": "00:03:08.309000",
    "text": "now the pattern of increasing the number\nof filters as the network becomes deeper"
  },
  {
    "start": "00:03:08.319000",
    "end": "00:03:11.830000",
    "text": "of filters as the network becomes deeper\nand deeper and uh at some point after"
  },
  {
    "start": "00:03:11.840000",
    "end": "00:03:15.390000",
    "text": "and deeper and uh at some point after\none two three four layers four"
  },
  {
    "start": "00:03:15.400000",
    "end": "00:03:19.350000",
    "text": "one two three four layers four\nconvolutional layers we are going to uh"
  },
  {
    "start": "00:03:19.360000",
    "end": "00:03:21.990000",
    "text": "convolutional layers we are going to uh\nhave the head and uh I think it's"
  },
  {
    "start": "00:03:22.000000",
    "end": "00:03:25.229000",
    "text": "have the head and uh I think it's\nworthwhile going back into this uh vgg"
  },
  {
    "start": "00:03:25.239000",
    "end": "00:03:27.990000",
    "text": "worthwhile going back into this uh vgg\nkind of architecture and look exactly"
  },
  {
    "start": "00:03:28.000000",
    "end": "00:03:30.670000",
    "text": "kind of architecture and look exactly\nwhere that head was in that architecture"
  },
  {
    "start": "00:03:30.680000",
    "end": "00:03:33.990000",
    "text": "where that head was in that architecture\nand and couple it with with this code um"
  },
  {
    "start": "00:03:34.000000",
    "end": "00:03:36.589000",
    "text": "and and couple it with with this code um\nso here is the the point where the head"
  },
  {
    "start": "00:03:36.599000",
    "end": "00:03:38.949000",
    "text": "so here is the the point where the head\nstarts and the head in this case is a"
  },
  {
    "start": "00:03:38.959000",
    "end": "00:03:41.750000",
    "text": "starts and the head in this case is a\nconcatenation of fully connected layers"
  },
  {
    "start": "00:03:41.760000",
    "end": "00:03:43.589000",
    "text": "concatenation of fully connected layers\nwhy we have this kind of concatenation"
  },
  {
    "start": "00:03:43.599000",
    "end": "00:03:47.949000",
    "text": "why we have this kind of concatenation\nand want do just a single layer um is uh"
  },
  {
    "start": "00:03:47.959000",
    "end": "00:03:50.070000",
    "text": "and want do just a single layer um is uh\nyou know gradually even within the head"
  },
  {
    "start": "00:03:50.080000",
    "end": "00:03:53.350000",
    "text": "you know gradually even within the head\nwe need to gradually reach this point of"
  },
  {
    "start": "00:03:53.360000",
    "end": "00:03:55.789000",
    "text": "we need to gradually reach this point of\nuh desired number of classes we have a"
  },
  {
    "start": "00:03:55.799000",
    "end": "00:03:59.949000",
    "text": "uh desired number of classes we have a\nclassification use case here um this is"
  },
  {
    "start": "00:03:59.959000",
    "end": "00:04:02.069000",
    "text": "classification use case here um this is\na thousand classes that are need to be"
  },
  {
    "start": "00:04:02.079000",
    "end": "00:04:05.030000",
    "text": "a thousand classes that are need to be\npresent in at the top of the at the end"
  },
  {
    "start": "00:04:05.040000",
    "end": "00:04:08.670000",
    "text": "present in at the top of the at the end\nof the of this U Network and this is"
  },
  {
    "start": "00:04:08.680000",
    "end": "00:04:10.229000",
    "text": "of the of this U Network and this is\nbasically the dimensionality of our"
  },
  {
    "start": "00:04:10.239000",
    "end": "00:04:13.429000",
    "text": "basically the dimensionality of our\nposterior probability uh distribution uh"
  },
  {
    "start": "00:04:13.439000",
    "end": "00:04:15.630000",
    "text": "posterior probability uh distribution uh\nwe're going to have the a y hat if you"
  },
  {
    "start": "00:04:15.640000",
    "end": "00:04:19.030000",
    "text": "we're going to have the a y hat if you\nlike that consist of a thousand numbers"
  },
  {
    "start": "00:04:19.040000",
    "end": "00:04:21.390000",
    "text": "like that consist of a thousand numbers\nuh a thousand are also the are the"
  },
  {
    "start": "00:04:21.400000",
    "end": "00:04:23.310000",
    "text": "uh a thousand are also the are the\nnumber of classes in the image net data"
  },
  {
    "start": "00:04:23.320000",
    "end": "00:04:26.790000",
    "text": "number of classes in the image net data\nset so this this Dimensions correspond"
  },
  {
    "start": "00:04:26.800000",
    "end": "00:04:30.510000",
    "text": "set so this this Dimensions correspond\nto the image net uh classifier uh data"
  },
  {
    "start": "00:04:30.520000",
    "end": "00:04:33.749000",
    "text": "to the image net uh classifier uh data\nset and so that's basically our head uh"
  },
  {
    "start": "00:04:33.759000",
    "end": "00:04:37.150000",
    "text": "set and so that's basically our head uh\nthere is uh also seen over here in this"
  },
  {
    "start": "00:04:37.160000",
    "end": "00:04:40.670000",
    "text": "there is uh also seen over here in this\ncode with uh this portion of the model"
  },
  {
    "start": "00:04:40.680000",
    "end": "00:04:43.909000",
    "text": "code with uh this portion of the model\nso we have whatever we have produced in"
  },
  {
    "start": "00:04:43.919000",
    "end": "00:04:46.110000",
    "text": "so we have whatever we have produced in\nterms of convolutions over here and then"
  },
  {
    "start": "00:04:46.120000",
    "end": "00:04:49.749000",
    "text": "terms of convolutions over here and then\nwe flatten the network so we flatten oh"
  },
  {
    "start": "00:04:49.759000",
    "end": "00:04:51.990000",
    "text": "we flatten the network so we flatten oh\nsorry flatten the output feature map"
  },
  {
    "start": "00:04:52.000000",
    "end": "00:04:54.230000",
    "text": "sorry flatten the output feature map\nthere by flattening the output F map we"
  },
  {
    "start": "00:04:54.240000",
    "end": "00:04:56.710000",
    "text": "there by flattening the output F map we\nare creating effectively uh a volume"
  },
  {
    "start": "00:04:56.720000",
    "end": "00:04:58.270000",
    "text": "are creating effectively uh a volume\nwe're taking a volume at the input and"
  },
  {
    "start": "00:04:58.280000",
    "end": "00:05:00.350000",
    "text": "we're taking a volume at the input and\nwe're flattening into a vector"
  },
  {
    "start": "00:05:00.360000",
    "end": "00:05:04.550000",
    "text": "we're flattening into a vector\nVOR and uh this Vector then is passed as"
  },
  {
    "start": "00:05:04.560000",
    "end": "00:05:05.950000",
    "text": "VOR and uh this Vector then is passed as\ninput"
  },
  {
    "start": "00:05:05.960000",
    "end": "00:05:09.510000",
    "text": "input\nto two dense layers the first dense"
  },
  {
    "start": "00:05:09.520000",
    "end": "00:05:14.110000",
    "text": "to two dense layers the first dense\nlayer is has 512 neurons it takes"
  },
  {
    "start": "00:05:14.120000",
    "end": "00:05:16.270000",
    "text": "layer is has 512 neurons it takes\nwhatever dimensionality uh and we'll see"
  },
  {
    "start": "00:05:16.280000",
    "end": "00:05:18.309000",
    "text": "whatever dimensionality uh and we'll see\nnow the dimensions in a moment uh the"
  },
  {
    "start": "00:05:18.319000",
    "end": "00:05:21.590000",
    "text": "now the dimensions in a moment uh the\nflatten layer provided and reduces that"
  },
  {
    "start": "00:05:21.600000",
    "end": "00:05:23.390000",
    "text": "flatten layer provided and reduces that\njust like any fully connected layer we"
  },
  {
    "start": "00:05:23.400000",
    "end": "00:05:25.350000",
    "text": "just like any fully connected layer we\nhave seen in a corresponding video in a"
  },
  {
    "start": "00:05:25.360000",
    "end": "00:05:29.230000",
    "text": "have seen in a corresponding video in a\ndifferent video uh earlier into 512"
  },
  {
    "start": "00:05:29.240000",
    "end": "00:05:30.309000",
    "text": "different video uh earlier into 512\ndimensions"
  },
  {
    "start": "00:05:30.319000",
    "end": "00:05:32.670000",
    "text": "dimensions\nand we use the rectified linear unit for"
  },
  {
    "start": "00:05:32.680000",
    "end": "00:05:35.830000",
    "text": "and we use the rectified linear unit for\nthat and then with the subsequent layer"
  },
  {
    "start": "00:05:35.840000",
    "end": "00:05:39.110000",
    "text": "that and then with the subsequent layer\ntakes 512 dimensions and reduces it"
  },
  {
    "start": "00:05:39.120000",
    "end": "00:05:42.270000",
    "text": "takes 512 dimensions and reduces it\nfurther into gas into a single uh"
  },
  {
    "start": "00:05:42.280000",
    "end": "00:05:44.350000",
    "text": "further into gas into a single uh\nDimension because as we have seen in the"
  },
  {
    "start": "00:05:44.360000",
    "end": "00:05:46.110000",
    "text": "Dimension because as we have seen in the\nbinary classification we have a binary"
  },
  {
    "start": "00:05:46.120000",
    "end": "00:05:48.270000",
    "text": "binary classification we have a binary\nclassification use case here either"
  },
  {
    "start": "00:05:48.280000",
    "end": "00:05:50.830000",
    "text": "classification use case here either\nwe're going to have a cats or dogs we"
  },
  {
    "start": "00:05:50.840000",
    "end": "00:05:54.469000",
    "text": "we're going to have a cats or dogs we\nhave um uh just a scaler that we need"
  },
  {
    "start": "00:05:54.479000",
    "end": "00:05:57.710000",
    "text": "have um uh just a scaler that we need\nbecause that is the probability of the"
  },
  {
    "start": "00:05:57.720000",
    "end": "00:05:59.670000",
    "text": "because that is the probability of the\npositive glass whatever that positive"
  },
  {
    "start": "00:05:59.680000",
    "end": "00:06:02.909000",
    "text": "positive glass whatever that positive\nclasses probably the dogs here uh and uh"
  },
  {
    "start": "00:06:02.919000",
    "end": "00:06:04.870000",
    "text": "classes probably the dogs here uh and uh\nwe are of course going to be using"
  },
  {
    "start": "00:06:04.880000",
    "end": "00:06:07.270000",
    "text": "we are of course going to be using\nsigmoid uh because only at the output of"
  },
  {
    "start": "00:06:07.280000",
    "end": "00:06:09.870000",
    "text": "sigmoid uh because only at the output of\nthe sigmoid we are actually getting this"
  },
  {
    "start": "00:06:09.880000",
    "end": "00:06:11.950000",
    "text": "the sigmoid we are actually getting this\nuh form of the posterior probability as"
  },
  {
    "start": "00:06:11.960000",
    "end": "00:06:14.350000",
    "text": "uh form of the posterior probability as\nwe had discussed in the uh fully"
  },
  {
    "start": "00:06:14.360000",
    "end": "00:06:17.469000",
    "text": "we had discussed in the uh fully\nconnected layers uh and in that in that"
  },
  {
    "start": "00:06:17.479000",
    "end": "00:06:19.589000",
    "text": "connected layers uh and in that in that\nuh lecture all right so this is"
  },
  {
    "start": "00:06:19.599000",
    "end": "00:06:21.790000",
    "text": "uh lecture all right so this is\nbasically our architecture uh very"
  },
  {
    "start": "00:06:21.800000",
    "end": "00:06:24.309000",
    "text": "basically our architecture uh very\nsimple architecture uh the convolution"
  },
  {
    "start": "00:06:24.319000",
    "end": "00:06:26.670000",
    "text": "simple architecture uh the convolution\nportions the flatten and the fully"
  },
  {
    "start": "00:06:26.680000",
    "end": "00:06:29.670000",
    "text": "portions the flatten and the fully\nconnected or dense uh portion to provide"
  },
  {
    "start": "00:06:29.680000",
    "end": "00:06:31.870000",
    "text": "connected or dense uh portion to provide\nthe binary classification result at the"
  },
  {
    "start": "00:06:31.880000",
    "end": "00:06:37.110000",
    "text": "the binary classification result at the\noutput and here is the U details of"
  },
  {
    "start": "00:06:37.120000",
    "end": "00:06:43.309000",
    "text": "output and here is the U details of\num of our uh CNN so we can see uh the"
  },
  {
    "start": "00:06:43.319000",
    "end": "00:06:46.350000",
    "text": "um of our uh CNN so we can see uh the\ninput uh images that are actually we"
  },
  {
    "start": "00:06:46.360000",
    "end": "00:06:50.950000",
    "text": "input uh images that are actually we\ncoming in uh the uh first uh we have 32"
  },
  {
    "start": "00:06:50.960000",
    "end": "00:06:53.670000",
    "text": "coming in uh the uh first uh we have 32\nfilters as we discussed in terms of"
  },
  {
    "start": "00:06:53.680000",
    "end": "00:06:54.990000",
    "text": "filters as we discussed in terms of\nnumber of"
  },
  {
    "start": "00:06:55.000000",
    "end": "00:06:57.749000",
    "text": "number of\nparameters um"
  },
  {
    "start": "00:06:57.759000",
    "end": "00:07:01.790000",
    "text": "parameters um\n896 18,000"
  },
  {
    "start": "00:07:05.720000",
    "end": "00:07:08.150000",
    "text": "73,000 147,000 so all of these are\nparameters that you see being quoted"
  },
  {
    "start": "00:07:08.160000",
    "end": "00:07:11.510000",
    "text": "parameters that you see being quoted\nhere in the uh next to the convolutional"
  },
  {
    "start": "00:07:11.520000",
    "end": "00:07:14.629000",
    "text": "here in the uh next to the convolutional\nlayers but um the most striking thing"
  },
  {
    "start": "00:07:14.639000",
    "end": "00:07:17.869000",
    "text": "layers but um the most striking thing\nover here is this look at the number of"
  },
  {
    "start": "00:07:17.879000",
    "end": "00:07:20.070000",
    "text": "over here is this look at the number of\nparameters which are involved in the"
  },
  {
    "start": "00:07:20.080000",
    "end": "00:07:22.990000",
    "text": "parameters which are involved in the\nfully connected in one fully connected"
  },
  {
    "start": "00:07:23.000000",
    "end": "00:07:26.510000",
    "text": "fully connected in one fully connected\nor dense layer 3.2 million parameters so"
  },
  {
    "start": "00:07:26.520000",
    "end": "00:07:29.110000",
    "text": "or dense layer 3.2 million parameters so\nout of the total 3 and A2 million"
  },
  {
    "start": "00:07:29.120000",
    "end": "00:07:32.230000",
    "text": "out of the total 3 and A2 million\nparameters that we have 3.2 million are"
  },
  {
    "start": "00:07:32.240000",
    "end": "00:07:34.510000",
    "text": "parameters that we have 3.2 million are\nassociated with a fully connected layer"
  },
  {
    "start": "00:07:34.520000",
    "end": "00:07:38.189000",
    "text": "associated with a fully connected layer\nand here is the kind of uh striking"
  },
  {
    "start": "00:07:38.199000",
    "end": "00:07:42.309000",
    "text": "and here is the kind of uh striking\nexample of um why it would make sense to"
  },
  {
    "start": "00:07:42.319000",
    "end": "00:07:45.629000",
    "text": "example of um why it would make sense to\nactually uh use CNN for uh image"
  },
  {
    "start": "00:07:45.639000",
    "end": "00:07:48.950000",
    "text": "actually uh use CNN for uh image\nclassification if we didn't have the CNN"
  },
  {
    "start": "00:07:48.960000",
    "end": "00:07:51.710000",
    "text": "classification if we didn't have the CNN\nand the associated advantage of that CNN"
  },
  {
    "start": "00:07:51.720000",
    "end": "00:07:55.029000",
    "text": "and the associated advantage of that CNN\nprovide which was actually also shown in"
  },
  {
    "start": "00:07:55.039000",
    "end": "00:07:57.230000",
    "text": "provide which was actually also shown in\nthis kind of snapshot architecture as"
  },
  {
    "start": "00:07:57.240000",
    "end": "00:08:02.510000",
    "text": "this kind of snapshot architecture as\nyou can see only the Loc"
  },
  {
    "start": "00:08:06.479000",
    "end": "00:08:09.230000",
    "text": "pixels the the one which are which are\nlocal to the uh special dimensions of"
  },
  {
    "start": "00:08:09.240000",
    "end": "00:08:12.270000",
    "text": "local to the uh special dimensions of\nthe filter are so so-called firing in"
  },
  {
    "start": "00:08:12.280000",
    "end": "00:08:15.629000",
    "text": "the filter are so so-called firing in\norder to produce that kind of scaler"
  },
  {
    "start": "00:08:15.639000",
    "end": "00:08:17.589000",
    "text": "order to produce that kind of scaler\nokay as compared to a fully connected"
  },
  {
    "start": "00:08:17.599000",
    "end": "00:08:19.309000",
    "text": "okay as compared to a fully connected\narchitecture where everything that we"
  },
  {
    "start": "00:08:19.319000",
    "end": "00:08:22.149000",
    "text": "architecture where everything that we\nhave here is going to be connected to"
  },
  {
    "start": "00:08:22.159000",
    "end": "00:08:24.950000",
    "text": "have here is going to be connected to\nthe layer um to to to form if you like"
  },
  {
    "start": "00:08:24.960000",
    "end": "00:08:28.589000",
    "text": "the layer um to to to form if you like\nthe output scaler z uh the convolutions"
  },
  {
    "start": "00:08:28.599000",
    "end": "00:08:31.309000",
    "text": "the output scaler z uh the convolutions\nuh are operation is actually helping us"
  },
  {
    "start": "00:08:31.319000",
    "end": "00:08:34.029000",
    "text": "uh are operation is actually helping us\nto significantly reduce the number of"
  },
  {
    "start": "00:08:34.039000",
    "end": "00:08:37.909000",
    "text": "to significantly reduce the number of\nparameters so uh at the end of the day"
  },
  {
    "start": "00:08:37.919000",
    "end": "00:08:42.110000",
    "text": "parameters so uh at the end of the day\nuh we have u u the scalar that indicates"
  },
  {
    "start": "00:08:42.120000",
    "end": "00:08:43.630000",
    "text": "uh we have u u the scalar that indicates\nthe posterior probability of the"
  },
  {
    "start": "00:08:43.640000",
    "end": "00:08:46.470000",
    "text": "the posterior probability of the\npositive class as we discussed and then"
  },
  {
    "start": "00:08:46.480000",
    "end": "00:08:50.190000",
    "text": "positive class as we discussed and then\nuh the architecture is seems to be uh"
  },
  {
    "start": "00:08:50.200000",
    "end": "00:08:54.310000",
    "text": "uh the architecture is seems to be uh\nvalid uh we are going to evidently going"
  },
  {
    "start": "00:08:54.320000",
    "end": "00:08:57.269000",
    "text": "valid uh we are going to evidently going\nto use binary cross entropy just like"
  },
  {
    "start": "00:08:57.279000",
    "end": "00:08:59.870000",
    "text": "to use binary cross entropy just like\nwhat we have done earlier in uh that"
  },
  {
    "start": "00:08:59.880000",
    "end": "00:09:01.870000",
    "text": "what we have done earlier in uh that\nother video where we looked at dense"
  },
  {
    "start": "00:09:01.880000",
    "end": "00:09:04.269000",
    "text": "other video where we looked at dense\nlayers only for binary classification or"
  },
  {
    "start": "00:09:04.279000",
    "end": "00:09:05.710000",
    "text": "layers only for binary classification or\nmulticlass"
  },
  {
    "start": "00:09:05.720000",
    "end": "00:09:08.030000",
    "text": "multiclass\nclassification uh and uh we are going to"
  },
  {
    "start": "00:09:08.040000",
    "end": "00:09:10.710000",
    "text": "classification uh and uh we are going to\nhave here well here the author selected"
  },
  {
    "start": "00:09:10.720000",
    "end": "00:09:13.350000",
    "text": "have here well here the author selected\nthe uh RMS prop which"
  },
  {
    "start": "00:09:13.360000",
    "end": "00:09:16.269000",
    "text": "the uh RMS prop which\nis uh one of the cousins of stochastic"
  },
  {
    "start": "00:09:16.279000",
    "end": "00:09:19.069000",
    "text": "is uh one of the cousins of stochastic\ngr descent we haven't really got any"
  },
  {
    "start": "00:09:19.079000",
    "end": "00:09:21.030000",
    "text": "gr descent we haven't really got any\ndiscussion specifically on enhancements"
  },
  {
    "start": "00:09:21.040000",
    "end": "00:09:23.470000",
    "text": "discussion specifically on enhancements\nof stochastic gr descent but if you do"
  },
  {
    "start": "00:09:23.480000",
    "end": "00:09:25.990000",
    "text": "of stochastic gr descent but if you do\nreplace it with SGD I think you will be"
  },
  {
    "start": "00:09:26.000000",
    "end": "00:09:29.030000",
    "text": "replace it with SGD I think you will be\ngetting very similar performance um with"
  },
  {
    "start": "00:09:29.040000",
    "end": "00:09:31.350000",
    "text": "getting very similar performance um with\nthe corresponding learning parameter and"
  },
  {
    "start": "00:09:31.360000",
    "end": "00:09:33.350000",
    "text": "the corresponding learning parameter and\nthen of course the metric is our"
  },
  {
    "start": "00:09:33.360000",
    "end": "00:09:35.550000",
    "text": "then of course the metric is our\naccuracy and one of the things that we"
  },
  {
    "start": "00:09:35.560000",
    "end": "00:09:38.350000",
    "text": "accuracy and one of the things that we\nwould like to point out in U in this"
  },
  {
    "start": "00:09:38.360000",
    "end": "00:09:40.310000",
    "text": "would like to point out in U in this\nkind of uh convolution and networks is"
  },
  {
    "start": "00:09:40.320000",
    "end": "00:09:42.990000",
    "text": "kind of uh convolution and networks is\nthat we will need to do to be careful"
  },
  {
    "start": "00:09:43.000000",
    "end": "00:09:45.389000",
    "text": "that we will need to do to be careful\nwhen we uh first take a data set and we"
  },
  {
    "start": "00:09:45.399000",
    "end": "00:09:48.750000",
    "text": "when we uh first take a data set and we\ntry to process the images as we have"
  },
  {
    "start": "00:09:48.760000",
    "end": "00:09:51.350000",
    "text": "try to process the images as we have\nseen the images are typically given to"
  },
  {
    "start": "00:09:51.360000",
    "end": "00:09:53.870000",
    "text": "seen the images are typically given to\nus as uh with pixels corresponds to"
  },
  {
    "start": "00:09:53.880000",
    "end": "00:09:56.030000",
    "text": "us as uh with pixels corresponds to\ninteger numbers so we have to uh"
  },
  {
    "start": "00:09:56.040000",
    "end": "00:09:59.069000",
    "text": "integer numbers so we have to uh\ndefinitely normalize them uh we have to"
  },
  {
    "start": "00:09:59.079000",
    "end": "00:10:01.310000",
    "text": "definitely normalize them uh we have to\nB them we have to do a lot of this kind"
  },
  {
    "start": "00:10:01.320000",
    "end": "00:10:04.310000",
    "text": "B them we have to do a lot of this kind\nof transformations in order for us to"
  },
  {
    "start": "00:10:04.320000",
    "end": "00:10:07.630000",
    "text": "of transformations in order for us to\nproduce uh the um the right uh inputs"
  },
  {
    "start": "00:10:07.640000",
    "end": "00:10:11.110000",
    "text": "produce uh the um the right uh inputs\nfor the uh for for our Network so after"
  },
  {
    "start": "00:10:11.120000",
    "end": "00:10:13.190000",
    "text": "for the uh for for our Network so after\na training process that involves"
  },
  {
    "start": "00:10:13.200000",
    "end": "00:10:15.910000",
    "text": "a training process that involves\nmultiple epochs as we would expect we"
  },
  {
    "start": "00:10:15.920000",
    "end": "00:10:19.389000",
    "text": "multiple epochs as we would expect we\nhave a model and uh we can actually plot"
  },
  {
    "start": "00:10:19.399000",
    "end": "00:10:21.710000",
    "text": "have a model and uh we can actually plot\nthe uh training and"
  },
  {
    "start": "00:10:21.720000",
    "end": "00:10:23.630000",
    "text": "the uh training and\nvalidation uh"
  },
  {
    "start": "00:10:23.640000",
    "end": "00:10:26.509000",
    "text": "validation uh\nloss as well also the corresponding kind"
  },
  {
    "start": "00:10:26.519000",
    "end": "00:10:29.310000",
    "text": "loss as well also the corresponding kind\nof accuracy and look at the corresp"
  },
  {
    "start": "00:10:29.320000",
    "end": "00:10:31.750000",
    "text": "of accuracy and look at the corresp\nresponding loss over here plot as the"
  },
  {
    "start": "00:10:31.760000",
    "end": "00:10:35.389000",
    "text": "responding loss over here plot as the\nnumber of epochs and uh remember what we"
  },
  {
    "start": "00:10:35.399000",
    "end": "00:10:38.230000",
    "text": "number of epochs and uh remember what we\nhave said in at the another video"
  },
  {
    "start": "00:10:38.240000",
    "end": "00:10:41.430000",
    "text": "have said in at the another video\nregarding uh the uh condition of"
  },
  {
    "start": "00:10:41.440000",
    "end": "00:10:43.430000",
    "text": "regarding uh the uh condition of\noverfitting and at that time the"
  },
  {
    "start": "00:10:43.440000",
    "end": "00:10:46.590000",
    "text": "overfitting and at that time the\ndiscussion was an example of a linear"
  },
  {
    "start": "00:10:46.600000",
    "end": "00:10:49.190000",
    "text": "discussion was an example of a linear\nmodel on the regression task over here"
  },
  {
    "start": "00:10:49.200000",
    "end": "00:10:51.990000",
    "text": "model on the regression task over here\nwe have a classification task but the"
  },
  {
    "start": "00:10:52.000000",
    "end": "00:10:53.910000",
    "text": "we have a classification task but the\nthe sort of problem of over fitting is"
  },
  {
    "start": "00:10:53.920000",
    "end": "00:10:57.350000",
    "text": "the sort of problem of over fitting is\npresent in across tasks in in in machine"
  },
  {
    "start": "00:10:57.360000",
    "end": "00:10:59.910000",
    "text": "present in across tasks in in in machine\nlearning so we see some quite"
  },
  {
    "start": "00:10:59.920000",
    "end": "00:11:02.509000",
    "text": "learning so we see some quite\nsignificant difference uh between"
  },
  {
    "start": "00:11:02.519000",
    "end": "00:11:05.670000",
    "text": "significant difference uh between\ntraining and validation as the accuracy"
  },
  {
    "start": "00:11:05.680000",
    "end": "00:11:06.550000",
    "text": "training and validation as the accuracy\nis"
  },
  {
    "start": "00:11:06.560000",
    "end": "00:11:08.910000",
    "text": "is\nimproving and that is really what we"
  },
  {
    "start": "00:11:08.920000",
    "end": "00:11:12.470000",
    "text": "improving and that is really what we\nhave said earlier as an a good indicator"
  },
  {
    "start": "00:11:12.480000",
    "end": "00:11:15.750000",
    "text": "have said earlier as an a good indicator\nof overfitting okay so uh it seems that"
  },
  {
    "start": "00:11:15.760000",
    "end": "00:11:18.269000",
    "text": "of overfitting okay so uh it seems that\nthe um Network that we have designed"
  },
  {
    "start": "00:11:18.279000",
    "end": "00:11:20.350000",
    "text": "the um Network that we have designed\nover here overfits the data set we are"
  },
  {
    "start": "00:11:20.360000",
    "end": "00:11:22.269000",
    "text": "over here overfits the data set we are\ngiven and it shouldn't be a complete"
  },
  {
    "start": "00:11:22.279000",
    "end": "00:11:24.710000",
    "text": "given and it shouldn't be a complete\nsurprise to us given the fact that we"
  },
  {
    "start": "00:11:24.720000",
    "end": "00:11:27.389000",
    "text": "surprise to us given the fact that we\nare throwing a significant number of"
  },
  {
    "start": "00:11:27.399000",
    "end": "00:11:30.350000",
    "text": "are throwing a significant number of\nparameters um in um"
  },
  {
    "start": "00:11:30.360000",
    "end": "00:11:33.030000",
    "text": "parameters um in um\nin a network in a data set which only"
  },
  {
    "start": "00:11:33.040000",
    "end": "00:11:37.150000",
    "text": "in a network in a data set which only\nhas a th000 labels per class and so uh"
  },
  {
    "start": "00:11:37.160000",
    "end": "00:11:39.069000",
    "text": "has a th000 labels per class and so uh\nwe can actually engage any of the"
  },
  {
    "start": "00:11:39.079000",
    "end": "00:11:41.110000",
    "text": "we can actually engage any of the\ntechniques that we have seen in"
  },
  {
    "start": "00:11:41.120000",
    "end": "00:11:43.430000",
    "text": "techniques that we have seen in\noverfitting uh to address overfitting"
  },
  {
    "start": "00:11:43.440000",
    "end": "00:11:45.470000",
    "text": "overfitting uh to address overfitting\nsuch as weight Decay any of the"
  },
  {
    "start": "00:11:45.480000",
    "end": "00:11:47.550000",
    "text": "such as weight Decay any of the\nregularization techniques that we have"
  },
  {
    "start": "00:11:47.560000",
    "end": "00:11:49.230000",
    "text": "regularization techniques that we have\nseen also in neuron networks to to"
  },
  {
    "start": "00:11:49.240000",
    "end": "00:11:51.030000",
    "text": "seen also in neuron networks to to\naddress it but in computer vision we"
  },
  {
    "start": "00:11:51.040000",
    "end": "00:11:53.389000",
    "text": "address it but in computer vision we\nhave something else that could actually"
  },
  {
    "start": "00:11:53.399000",
    "end": "00:11:56.269000",
    "text": "have something else that could actually\nhelp us and this is actually called the"
  },
  {
    "start": "00:11:56.279000",
    "end": "00:11:58.550000",
    "text": "help us and this is actually called the\ndocumentation so I think it's worthwhile"
  },
  {
    "start": "00:11:58.560000",
    "end": "00:12:00.150000",
    "text": "documentation so I think it's worthwhile\ngoing through"
  },
  {
    "start": "00:12:00.160000",
    "end": "00:12:03.190000",
    "text": "going through\nuh the data augmentation because it is"
  },
  {
    "start": "00:12:03.200000",
    "end": "00:12:05.310000",
    "text": "uh the data augmentation because it is\nreally a fairly straightforward and"
  },
  {
    "start": "00:12:05.320000",
    "end": "00:12:07.710000",
    "text": "really a fairly straightforward and\nwidely used approach to avoid the"
  },
  {
    "start": "00:12:07.720000",
    "end": "00:12:09.470000",
    "text": "widely used approach to avoid the\nsituation such as this where we have"
  },
  {
    "start": "00:12:09.480000",
    "end": "00:12:11.790000",
    "text": "situation such as this where we have\noverfeeding so in that augmentation what"
  },
  {
    "start": "00:12:11.800000",
    "end": "00:12:14.509000",
    "text": "overfeeding so in that augmentation what\nwe actually do we are taking the input"
  },
  {
    "start": "00:12:14.519000",
    "end": "00:12:16.910000",
    "text": "we actually do we are taking the input\nimages and given the fact that we have"
  },
  {
    "start": "00:12:16.920000",
    "end": "00:12:19.030000",
    "text": "images and given the fact that we have\nthe knowledge of the class we try to"
  },
  {
    "start": "00:12:19.040000",
    "end": "00:12:21.470000",
    "text": "the knowledge of the class we try to\ntransform these input images in creating"
  },
  {
    "start": "00:12:21.480000",
    "end": "00:12:24.150000",
    "text": "transform these input images in creating\nmore data so that's the an artificial"
  },
  {
    "start": "00:12:24.160000",
    "end": "00:12:25.990000",
    "text": "more data so that's the an artificial\nway of increasing the number of labels"
  },
  {
    "start": "00:12:26.000000",
    "end": "00:12:28.670000",
    "text": "way of increasing the number of labels\nwe have in our data set we we have"
  },
  {
    "start": "00:12:28.680000",
    "end": "00:12:30.509000",
    "text": "we have in our data set we we have\nvarious kind of Transformations we may"
  },
  {
    "start": "00:12:30.519000",
    "end": "00:12:34.670000",
    "text": "various kind of Transformations we may\nbe shifting rotating images we may"
  },
  {
    "start": "00:12:34.680000",
    "end": "00:12:37.910000",
    "text": "be shifting rotating images we may\nsharing uh the image uh we have we are"
  },
  {
    "start": "00:12:37.920000",
    "end": "00:12:40.829000",
    "text": "sharing uh the image uh we have we are\nzooming in zooming out uh and uh"
  },
  {
    "start": "00:12:40.839000",
    "end": "00:12:43.509000",
    "text": "zooming in zooming out uh and uh\nflipping and so on we are definitely"
  },
  {
    "start": "00:12:43.519000",
    "end": "00:12:47.949000",
    "text": "flipping and so on we are definitely\ngoing to be creating some uh nasty uh"
  },
  {
    "start": "00:12:47.959000",
    "end": "00:12:51.509000",
    "text": "going to be creating some uh nasty uh\ncats um or dogs uh but definitely this"
  },
  {
    "start": "00:12:51.519000",
    "end": "00:12:55.230000",
    "text": "cats um or dogs uh but definitely this\nhelps our Network to not overfit and so"
  },
  {
    "start": "00:12:55.240000",
    "end": "00:12:58.150000",
    "text": "helps our Network to not overfit and so\nif you are to just keep the exactly the"
  },
  {
    "start": "00:12:58.160000",
    "end": "00:13:00.470000",
    "text": "if you are to just keep the exactly the\nsame network chitecture as we have seen"
  },
  {
    "start": "00:13:00.480000",
    "end": "00:13:03.509000",
    "text": "same network chitecture as we have seen\nearlier not not touch at all the model"
  },
  {
    "start": "00:13:03.519000",
    "end": "00:13:05.990000",
    "text": "earlier not not touch at all the model\nbut definitely train the model with this"
  },
  {
    "start": "00:13:06.000000",
    "end": "00:13:09.629000",
    "text": "but definitely train the model with this\nadditional kind of uh data set uh then"
  },
  {
    "start": "00:13:09.639000",
    "end": "00:13:13.509000",
    "text": "additional kind of uh data set uh then\nuh uh look what happened uh we have a"
  },
  {
    "start": "00:13:13.519000",
    "end": "00:13:16.310000",
    "text": "uh uh look what happened uh we have a\ntraining and validation loss which are"
  },
  {
    "start": "00:13:16.320000",
    "end": "00:13:18.509000",
    "text": "training and validation loss which are\nvery close to each other so we actually"
  },
  {
    "start": "00:13:18.519000",
    "end": "00:13:21.590000",
    "text": "very close to each other so we actually\nhave solved the uh overfeeding problem"
  },
  {
    "start": "00:13:21.600000",
    "end": "00:13:24.990000",
    "text": "have solved the uh overfeeding problem\nand our accuracy is uh both in terms of"
  },
  {
    "start": "00:13:25.000000",
    "end": "00:13:27.150000",
    "text": "and our accuracy is uh both in terms of\ntraining and validation are also very"
  },
  {
    "start": "00:13:27.160000",
    "end": "00:13:29.949000",
    "text": "training and validation are also very\nclose and close to some something like"
  },
  {
    "start": "00:13:29.959000",
    "end": "00:13:32.189000",
    "text": "close and close to some something like\n85% okay so I think this is a good"
  },
  {
    "start": "00:13:32.199000",
    "end": "00:13:37.710000",
    "text": "85% okay so I think this is a good\nexample to showcase the U uh CNN models"
  },
  {
    "start": "00:13:37.720000",
    "end": "00:13:40.310000",
    "text": "example to showcase the U uh CNN models\nuh as a as working for the simple task"
  },
  {
    "start": "00:13:40.320000",
    "end": "00:13:42.829000",
    "text": "uh as a as working for the simple task\nof image classification and what we"
  },
  {
    "start": "00:13:42.839000",
    "end": "00:13:45.269000",
    "text": "of image classification and what we\nactually also would like to understand"
  },
  {
    "start": "00:13:45.279000",
    "end": "00:13:48.310000",
    "text": "actually also would like to understand\nnow next is um what we have said earlier"
  },
  {
    "start": "00:13:48.320000",
    "end": "00:13:51.590000",
    "text": "now next is um what we have said earlier\nabout Hey what how can we have some kind"
  },
  {
    "start": "00:13:51.600000",
    "end": "00:13:54.910000",
    "text": "about Hey what how can we have some kind\nof visualization into the internals of"
  },
  {
    "start": "00:13:54.920000",
    "end": "00:13:57.749000",
    "text": "of visualization into the internals of\nthe CNN to understand what is uh what is"
  },
  {
    "start": "00:13:57.759000",
    "end": "00:13:59.670000",
    "text": "the CNN to understand what is uh what is\nactually learning and and this is what"
  },
  {
    "start": "00:13:59.680000",
    "end": "00:14:03.920000",
    "text": "actually learning and and this is what\nwe will be discussing next"
  }
]