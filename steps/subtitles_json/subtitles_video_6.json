[
  {
    "start": "00:00:02.919000",
    "end": "00:00:06.150000",
    "text": "in an earli video we have seen the uh\nconvolutional networks and the basic"
  },
  {
    "start": "00:00:06.160000",
    "end": "00:00:09.750000",
    "text": "convolutional networks and the basic\noperation uh in this uh video what we"
  },
  {
    "start": "00:00:09.760000",
    "end": "00:00:13.190000",
    "text": "operation uh in this uh video what we\nactually introducing here uh is residual"
  },
  {
    "start": "00:00:13.200000",
    "end": "00:00:16.550000",
    "text": "actually introducing here uh is residual\nnetworks which is to to this day um"
  },
  {
    "start": "00:00:16.560000",
    "end": "00:00:18.510000",
    "text": "networks which is to to this day um\nseveral years after the introduction"
  },
  {
    "start": "00:00:18.520000",
    "end": "00:00:21.189000",
    "text": "several years after the introduction\nremain one of the main uh used"
  },
  {
    "start": "00:00:21.199000",
    "end": "00:00:24.070000",
    "text": "remain one of the main uh used\narchitectures uh for feature extraction"
  },
  {
    "start": "00:00:24.080000",
    "end": "00:00:26.790000",
    "text": "architectures uh for feature extraction\nand not only as a basic component of"
  },
  {
    "start": "00:00:26.800000",
    "end": "00:00:29.710000",
    "text": "and not only as a basic component of\nmany more advanced uh CNN architectures"
  },
  {
    "start": "00:00:29.720000",
    "end": "00:00:32.630000",
    "text": "many more advanced uh CNN architectures\nand and uh that are doing more more"
  },
  {
    "start": "00:00:32.640000",
    "end": "00:00:35.069000",
    "text": "and and uh that are doing more more\ncomplicated tasks such as object"
  },
  {
    "start": "00:00:35.079000",
    "end": "00:00:37.430000",
    "text": "complicated tasks such as object\ndetection semantic segmentation and"
  },
  {
    "start": "00:00:37.440000",
    "end": "00:00:40.670000",
    "text": "detection semantic segmentation and\nothers that we will see in another uh in"
  },
  {
    "start": "00:00:40.680000",
    "end": "00:00:46.069000",
    "text": "others that we will see in another uh in\nanother video uh so the uh history if"
  },
  {
    "start": "00:00:46.079000",
    "end": "00:00:48.510000",
    "text": "another video uh so the uh history if\nyou like of their introduction uh you"
  },
  {
    "start": "00:00:48.520000",
    "end": "00:00:52.430000",
    "text": "you like of their introduction uh you\nknow started uh around 2015 where people"
  },
  {
    "start": "00:00:52.440000",
    "end": "00:00:56.069000",
    "text": "know started uh around 2015 where people\nrealized that it's um not really"
  },
  {
    "start": "00:00:56.079000",
    "end": "00:01:00.229000",
    "text": "realized that it's um not really\npossible to extend the so-called uh"
  },
  {
    "start": "00:01:00.239000",
    "end": "00:01:02.790000",
    "text": "possible to extend the so-called uh\narchitectures of the time let's say the"
  },
  {
    "start": "00:01:02.800000",
    "end": "00:01:05.950000",
    "text": "architectures of the time let's say the\nvgg architecture uh we've seen the V"
  },
  {
    "start": "00:01:05.960000",
    "end": "00:01:08.990000",
    "text": "vgg architecture uh we've seen the V\narchitecture on on a different U video"
  },
  {
    "start": "00:01:09.000000",
    "end": "00:01:11.469000",
    "text": "architecture on on a different U video\nearlier in this uh in this actually"
  },
  {
    "start": "00:01:11.479000",
    "end": "00:01:14.670000",
    "text": "earlier in this uh in this actually\ntopic over here uh where we have SE we"
  },
  {
    "start": "00:01:14.680000",
    "end": "00:01:18.550000",
    "text": "topic over here uh where we have SE we\nhave seen the sort of uh architecture of"
  },
  {
    "start": "00:01:18.560000",
    "end": "00:01:22.510000",
    "text": "have seen the sort of uh architecture of\nthe V16 Network and uh what was actually"
  },
  {
    "start": "00:01:22.520000",
    "end": "00:01:24.429000",
    "text": "the V16 Network and uh what was actually\nhappening then and now that we"
  },
  {
    "start": "00:01:24.439000",
    "end": "00:01:26.870000",
    "text": "happening then and now that we\nunderstand a couple of things about back"
  },
  {
    "start": "00:01:26.880000",
    "end": "00:01:30.550000",
    "text": "understand a couple of things about back\npropagation uh the uh gradient"
  },
  {
    "start": "00:01:30.560000",
    "end": "00:01:32.429000",
    "text": "propagation uh the uh gradient\nhad a lot of problems to"
  },
  {
    "start": "00:01:32.439000",
    "end": "00:01:35.429000",
    "text": "had a lot of problems to\nflow all the way to the input of the"
  },
  {
    "start": "00:01:35.439000",
    "end": "00:01:38.550000",
    "text": "flow all the way to the input of the\nnetwork uh and uh the U sort of"
  },
  {
    "start": "00:01:38.560000",
    "end": "00:01:40.670000",
    "text": "network uh and uh the U sort of\nbottlenecks that were actually generated"
  },
  {
    "start": "00:01:40.680000",
    "end": "00:01:42.590000",
    "text": "bottlenecks that were actually generated\ncreated a significant problems in the"
  },
  {
    "start": "00:01:42.600000",
    "end": "00:01:45.510000",
    "text": "created a significant problems in the\ntraining of these uh architectures so"
  },
  {
    "start": "00:01:45.520000",
    "end": "00:01:46.550000",
    "text": "training of these uh architectures so\naround"
  },
  {
    "start": "00:01:46.560000",
    "end": "00:01:49.630000",
    "text": "around\n2015 a researcher at"
  },
  {
    "start": "00:01:49.640000",
    "end": "00:01:52.830000",
    "text": "2015 a researcher at\nMicrosoft uh you know found a solution"
  },
  {
    "start": "00:01:52.840000",
    "end": "00:01:56.149000",
    "text": "Microsoft uh you know found a solution\non how to enable uh that gradient to"
  },
  {
    "start": "00:01:56.159000",
    "end": "00:01:57.670000",
    "text": "on how to enable uh that gradient to\nflow uh"
  },
  {
    "start": "00:01:57.680000",
    "end": "00:02:00.550000",
    "text": "flow uh\nfreely in uh in a much deeper"
  },
  {
    "start": "00:02:00.560000",
    "end": "00:02:02.789000",
    "text": "freely in uh in a much deeper\narchitectures such as uh the ones that"
  },
  {
    "start": "00:02:02.799000",
    "end": "00:02:05.670000",
    "text": "architectures such as uh the ones that\nwe will see in a moment and uh the uh"
  },
  {
    "start": "00:02:05.680000",
    "end": "00:02:08.749000",
    "text": "we will see in a moment and uh the uh\nthis gave their uh the name residual"
  },
  {
    "start": "00:02:08.759000",
    "end": "00:02:11.270000",
    "text": "this gave their uh the name residual\nbecause it in that architecture we"
  },
  {
    "start": "00:02:11.280000",
    "end": "00:02:13.550000",
    "text": "because it in that architecture we\nImplement what we call a residual unit"
  },
  {
    "start": "00:02:13.560000",
    "end": "00:02:16.190000",
    "text": "Implement what we call a residual unit\nand from from now on we'll refer to to"
  },
  {
    "start": "00:02:16.200000",
    "end": "00:02:19.670000",
    "text": "and from from now on we'll refer to to\nthose networks as rest nets all right so"
  },
  {
    "start": "00:02:19.680000",
    "end": "00:02:22.070000",
    "text": "those networks as rest nets all right so\nin order for us to understand what is"
  },
  {
    "start": "00:02:22.080000",
    "end": "00:02:23.630000",
    "text": "in order for us to understand what is\ngoing on with rest Nets or what I'm"
  },
  {
    "start": "00:02:23.640000",
    "end": "00:02:27.630000",
    "text": "going on with rest Nets or what I'm\ngoing to do now is I'm going to draw uh"
  },
  {
    "start": "00:02:27.640000",
    "end": "00:02:31.190000",
    "text": "going to do now is I'm going to draw uh\na very small rest architecture just"
  },
  {
    "start": "00:02:31.200000",
    "end": "00:02:33.150000",
    "text": "a very small rest architecture just\nconsisting of three"
  },
  {
    "start": "00:02:33.160000",
    "end": "00:02:37.030000",
    "text": "consisting of three\nunits U and if I may draw the"
  },
  {
    "start": "00:02:37.040000",
    "end": "00:02:41.030000",
    "text": "units U and if I may draw the\narchitecture will that's my uh unit"
  },
  {
    "start": "00:02:41.040000",
    "end": "00:02:45.350000",
    "text": "architecture will that's my uh unit\nwhich I'll abstract with the letter"
  },
  {
    "start": "00:02:45.360000",
    "end": "00:02:49.589000",
    "text": "which I'll abstract with the letter\nF1 so the input to the so I'll use a bit"
  },
  {
    "start": "00:02:49.599000",
    "end": "00:02:51.750000",
    "text": "F1 so the input to the so I'll use a bit\ndifferent terminology from what I was"
  },
  {
    "start": "00:02:51.760000",
    "end": "00:02:53.750000",
    "text": "different terminology from what I was\nused kind of earlier so I'll be calling"
  },
  {
    "start": "00:02:53.760000",
    "end": "00:02:57.630000",
    "text": "used kind of earlier so I'll be calling\nset of X I'll be calling it y0 I could"
  },
  {
    "start": "00:02:57.640000",
    "end": "00:03:00.229000",
    "text": "set of X I'll be calling it y0 I could\nhave used it also x0 but in my not over"
  },
  {
    "start": "00:03:00.239000",
    "end": "00:03:04.750000",
    "text": "have used it also x0 but in my not over\nhere I have uh the uh this as y z okay"
  },
  {
    "start": "00:03:04.760000",
    "end": "00:03:10.110000",
    "text": "here I have uh the uh this as y z okay\nso the y z goes into a block that will"
  },
  {
    "start": "00:03:10.120000",
    "end": "00:03:12.470000",
    "text": "so the y z goes into a block that will\nconsist of one or more convolutional"
  },
  {
    "start": "00:03:12.480000",
    "end": "00:03:13.350000",
    "text": "consist of one or more convolutional\nkind of"
  },
  {
    "start": "00:03:13.360000",
    "end": "00:03:16.190000",
    "text": "kind of\nlayers and in the residual architecture"
  },
  {
    "start": "00:03:16.200000",
    "end": "00:03:20.350000",
    "text": "layers and in the residual architecture\nand this is uh why we call it residuals"
  },
  {
    "start": "00:03:20.360000",
    "end": "00:03:22.710000",
    "text": "and this is uh why we call it residuals\nwe take the"
  },
  {
    "start": "00:03:22.720000",
    "end": "00:03:27.550000",
    "text": "we take the\ninput and add it with a"
  },
  {
    "start": "00:03:27.560000",
    "end": "00:03:31.910000",
    "text": "input and add it with a\nunit uh gain into to the output to the"
  },
  {
    "start": "00:03:31.920000",
    "end": "00:03:35.670000",
    "text": "unit uh gain into to the output to the\noutput okay so to form what we call now"
  },
  {
    "start": "00:03:35.680000",
    "end": "00:03:41.589000",
    "text": "output okay so to form what we call now\na y1 the y1 is uh being added"
  },
  {
    "start": "00:03:41.599000",
    "end": "00:03:49.350000",
    "text": "a y1 the y1 is uh being added\ninto again with exactly the same"
  },
  {
    "start": "00:03:59.959000",
    "end": "00:04:06.270000",
    "text": "output Y2\nand uh the Y2 is similarly"
  },
  {
    "start": "00:04:15.120000",
    "end": "00:04:17.430000",
    "text": "final Y3 output okay so this is the kind\nof a basic rest net kind of"
  },
  {
    "start": "00:04:17.440000",
    "end": "00:04:20.909000",
    "text": "of a basic rest net kind of\narchitecture uh in U and if you compare"
  },
  {
    "start": "00:04:20.919000",
    "end": "00:04:23.909000",
    "text": "architecture uh in U and if you compare\nwhat we have seen earlier with the cnns"
  },
  {
    "start": "00:04:23.919000",
    "end": "00:04:26.310000",
    "text": "what we have seen earlier with the cnns\nwe had convolutional layers Max pool"
  },
  {
    "start": "00:04:26.320000",
    "end": "00:04:28.510000",
    "text": "we had convolutional layers Max pool\nlayers and so nonlinear evidently over"
  },
  {
    "start": "00:04:28.520000",
    "end": "00:04:30.830000",
    "text": "layers and so nonlinear evidently over\nhere but we never had this kind of skip"
  },
  {
    "start": "00:04:30.840000",
    "end": "00:04:35.590000",
    "text": "here but we never had this kind of skip\nover connection uh as we call it okay so"
  },
  {
    "start": "00:04:35.600000",
    "end": "00:04:38.830000",
    "text": "over connection uh as we call it okay so\nI want to just uh go ahead and write now"
  },
  {
    "start": "00:04:38.840000",
    "end": "00:04:41.189000",
    "text": "I want to just uh go ahead and write now\nthe"
  },
  {
    "start": "00:04:41.199000",
    "end": "00:04:45.110000",
    "text": "the\num expression of the output of each of"
  },
  {
    "start": "00:04:45.120000",
    "end": "00:04:46.790000",
    "text": "um expression of the output of each of\nthese uh"
  },
  {
    "start": "00:04:46.800000",
    "end": "00:04:49.070000",
    "text": "these uh\nblocks with respect"
  },
  {
    "start": "00:04:49.080000",
    "end": "00:04:53.070000",
    "text": "blocks with respect\nto the input so I hope you"
  },
  {
    "start": "00:04:53.080000",
    "end": "00:04:56.270000",
    "text": "to the input so I hope you\nagree uh that this is exactly what each"
  },
  {
    "start": "00:04:56.280000",
    "end": "00:05:00.710000",
    "text": "agree uh that this is exactly what each\nof these blocks is"
  },
  {
    "start": "00:05:00.720000",
    "end": "00:05:05.430000",
    "text": "of these blocks is\nImplement okay so we have uh the FI of Y"
  },
  {
    "start": "00:05:05.440000",
    "end": "00:05:08.870000",
    "text": "Implement okay so we have uh the FI of Y\nIUS one plus the Y IUS one in each of"
  },
  {
    "start": "00:05:08.880000",
    "end": "00:05:11.189000",
    "text": "IUS one plus the Y IUS one in each of\nthese so if I may write down these"
  },
  {
    "start": "00:05:11.199000",
    "end": "00:05:14.990000",
    "text": "these so if I may write down these\nequations for uh let's say the"
  },
  {
    "start": "00:05:15.000000",
    "end": "00:05:20.350000",
    "text": "equations for uh let's say the\nY3 is equal to F3 of Y2 +"
  },
  {
    "start": "00:05:20.360000",
    "end": "00:05:24.990000",
    "text": "Y3 is equal to F3 of Y2 +\nY2 um the Y2"
  },
  {
    "start": "00:05:25.000000",
    "end": "00:05:30.230000",
    "text": "Y2 um the Y2\nitself is FS2 of y1 + y1"
  },
  {
    "start": "00:05:30.240000",
    "end": "00:05:36.749000",
    "text": "itself is FS2 of y1 + y1\nand the y1 is fub1 of y0 + y0 these are"
  },
  {
    "start": "00:05:36.759000",
    "end": "00:05:39.590000",
    "text": "and the y1 is fub1 of y0 + y0 these are\nthe three equations for each of the"
  },
  {
    "start": "00:05:39.600000",
    "end": "00:05:41.990000",
    "text": "the three equations for each of the\nthree blocks that I have here and what I"
  },
  {
    "start": "00:05:42.000000",
    "end": "00:05:44.029000",
    "text": "three blocks that I have here and what I\nwant to do now is I want to start"
  },
  {
    "start": "00:05:44.039000",
    "end": "00:05:49.230000",
    "text": "want to do now is I want to start\nreplacing the Y2 uh and y1 uh into the"
  },
  {
    "start": "00:05:49.240000",
    "end": "00:05:51.990000",
    "text": "replacing the Y2 uh and y1 uh into the\nequation Y3 because I want to write down"
  },
  {
    "start": "00:05:52.000000",
    "end": "00:05:55.909000",
    "text": "equation Y3 because I want to write down\nthe equation the the form of the Y3 as a"
  },
  {
    "start": "00:05:55.919000",
    "end": "00:06:02.749000",
    "text": "the equation the the form of the Y3 as a\nfunction of only the y0 and the two um"
  },
  {
    "start": "00:06:02.759000",
    "end": "00:06:04.870000",
    "text": "function of only the y0 and the two um\nfunctions that are involved in the"
  },
  {
    "start": "00:06:04.880000",
    "end": "00:06:08.070000",
    "text": "functions that are involved in the\nblocks F1 and F2 okay so all right so"
  },
  {
    "start": "00:06:08.080000",
    "end": "00:06:10.589000",
    "text": "blocks F1 and F2 okay so all right so\nwhat I'm going to do now is I'm going to"
  },
  {
    "start": "00:06:10.599000",
    "end": "00:06:12.110000",
    "text": "what I'm going to do now is I'm going to\nwrite it as"
  },
  {
    "start": "00:06:12.120000",
    "end": "00:06:15.309000",
    "text": "write it as\nF3 of"
  },
  {
    "start": "00:06:15.319000",
    "end": "00:06:22.270000",
    "text": "F3 of\nY2 I'm sorry F3 of"
  },
  {
    "start": "00:06:26.880000",
    "end": "00:06:31.950000",
    "text": "FS2 of y1 +\ny1 + FS2 of Y y 1 +"
  },
  {
    "start": "00:06:31.960000",
    "end": "00:06:35.990000",
    "text": "y1 + FS2 of Y y 1 +\ny1 okay so I just replace it Y2 with it"
  },
  {
    "start": "00:06:36.000000",
    "end": "00:06:46.990000",
    "text": "y1 okay so I just replace it Y2 with it\nequal and now I can uh uh replace"
  },
  {
    "start": "00:06:49.919000",
    "end": "00:06:54.029000",
    "text": "now let me write it over here because I\njust need a long line to replace"
  },
  {
    "start": "00:06:54.039000",
    "end": "00:06:56.990000",
    "text": "just need a long line to replace\nit to make the final replacement so it"
  },
  {
    "start": "00:06:57.000000",
    "end": "00:06:58.510000",
    "text": "it to make the final replacement so it\nis"
  },
  {
    "start": "00:06:58.520000",
    "end": "00:07:01.749000",
    "text": "is\nF3 of F2 now I'm going to replace the F"
  },
  {
    "start": "00:07:01.759000",
    "end": "00:07:07.629000",
    "text": "F3 of F2 now I'm going to replace the F\ny1 with its equal to uh obtain the"
  },
  {
    "start": "00:07:07.639000",
    "end": "00:07:18.790000",
    "text": "y1 with its equal to uh obtain the\nfinal"
  },
  {
    "start": "00:07:21.120000",
    "end": "00:07:26.469000",
    "text": "expression and this is now the second\nsquare"
  },
  {
    "start": "00:07:29.560000",
    "end": "00:07:32.550000",
    "text": "bracket okay so it is really this\nbracket over here"
  },
  {
    "start": "00:07:32.560000",
    "end": "00:07:34.550000",
    "text": "bracket over here\nplus"
  },
  {
    "start": "00:07:34.560000",
    "end": "00:07:36.909000",
    "text": "plus\nplus I have"
  },
  {
    "start": "00:07:36.919000",
    "end": "00:07:40.950000",
    "text": "plus I have\nFS2 of FS1 of y0 +"
  },
  {
    "start": "00:07:40.960000",
    "end": "00:07:43.230000",
    "text": "FS2 of FS1 of y0 +\ny0 plus"
  },
  {
    "start": "00:07:43.240000",
    "end": "00:07:46.350000",
    "text": "y0 plus\nfub1 of y0 + y"
  },
  {
    "start": "00:07:46.360000",
    "end": "00:07:51.749000",
    "text": "fub1 of y0 + y\nz okay so this is uh my final expression"
  },
  {
    "start": "00:07:51.759000",
    "end": "00:07:53.430000",
    "text": "z okay so this is uh my final expression\nwith respect"
  },
  {
    "start": "00:07:53.440000",
    "end": "00:07:58.070000",
    "text": "with respect\nto the output of Y3 and why I did that I"
  },
  {
    "start": "00:07:58.080000",
    "end": "00:08:00.589000",
    "text": "to the output of Y3 and why I did that I\nwant to take this kind of architecture"
  },
  {
    "start": "00:08:00.599000",
    "end": "00:08:04.309000",
    "text": "want to take this kind of architecture\nand write its equivalent that we just"
  },
  {
    "start": "00:08:04.319000",
    "end": "00:08:07.629000",
    "text": "and write its equivalent that we just\nbased on this equation and that kind of"
  },
  {
    "start": "00:08:07.639000",
    "end": "00:08:10.670000",
    "text": "based on this equation and that kind of\nre- plotting or redraw drawing of this"
  },
  {
    "start": "00:08:10.680000",
    "end": "00:08:12.350000",
    "text": "re- plotting or redraw drawing of this\nkind of architecture will help me kind"
  },
  {
    "start": "00:08:12.360000",
    "end": "00:08:14.350000",
    "text": "kind of architecture will help me kind\nof understand a couple of things about"
  },
  {
    "start": "00:08:14.360000",
    "end": "00:08:18.110000",
    "text": "of understand a couple of things about\nthe advantages of rest Nets and why they"
  },
  {
    "start": "00:08:18.120000",
    "end": "00:08:20.510000",
    "text": "the advantages of rest Nets and why they\nsolve the problem of gradient flow"
  },
  {
    "start": "00:08:20.520000",
    "end": "00:08:21.710000",
    "text": "solve the problem of gradient flow\nthroughout the"
  },
  {
    "start": "00:08:21.720000",
    "end": "00:08:25.710000",
    "text": "throughout the\nnetwork okay so I am going to start so"
  },
  {
    "start": "00:08:25.720000",
    "end": "00:08:29.990000",
    "text": "network okay so I am going to start so\nI'm dividing this into um two parts uh I"
  },
  {
    "start": "00:08:30.000000",
    "end": "00:08:33.149000",
    "text": "I'm dividing this into um two parts uh I\nam going to um"
  },
  {
    "start": "00:08:33.159000",
    "end": "00:08:36.589000",
    "text": "am going to um\nfirst uh draw"
  },
  {
    "start": "00:08:36.599000",
    "end": "00:08:40.509000",
    "text": "first uh draw\nthe long part over here this F3"
  },
  {
    "start": "00:08:40.519000",
    "end": "00:08:43.550000",
    "text": "the long part over here this F3\nexpression here at the at the bottom so"
  },
  {
    "start": "00:08:43.560000",
    "end": "00:08:47.430000",
    "text": "expression here at the at the bottom so\nI am going to take this accepts as input"
  },
  {
    "start": "00:08:47.440000",
    "end": "00:08:50.430000",
    "text": "I am going to take this accepts as input\ny0 that's the only input in this"
  },
  {
    "start": "00:08:50.440000",
    "end": "00:08:54.910000",
    "text": "y0 that's the only input in this\ndiagram um so y0 is going into the"
  },
  {
    "start": "00:08:54.920000",
    "end": "00:08:57.750000",
    "text": "diagram um so y0 is going into the\nfunction"
  },
  {
    "start": "00:09:00.240000",
    "end": "00:09:03.430000",
    "text": "F1 F1\nwe are adding now the function"
  },
  {
    "start": "00:09:03.440000",
    "end": "00:09:08.310000",
    "text": "we are adding now the function\nuh the the y z into it okay so that's"
  },
  {
    "start": "00:09:08.320000",
    "end": "00:09:11.710000",
    "text": "uh the the y z into it okay so that's\nbasically uh this"
  },
  {
    "start": "00:09:11.720000",
    "end": "00:09:14.750000",
    "text": "basically uh this\nterm all right so then we take the FS2"
  },
  {
    "start": "00:09:14.760000",
    "end": "00:09:16.870000",
    "text": "term all right so then we take the FS2\nof this term so the output of this is"
  },
  {
    "start": "00:09:16.880000",
    "end": "00:09:19.670000",
    "text": "of this term so the output of this is\nbeing fed to"
  },
  {
    "start": "00:09:19.680000",
    "end": "00:09:27.069000",
    "text": "being fed to\nF2 and uh what we do is uh we add for"
  },
  {
    "start": "00:09:27.079000",
    "end": "00:09:31.030000",
    "text": "F2 and uh what we do is uh we add for\nthis FS2 we add this term over here F1 y"
  },
  {
    "start": "00:09:31.040000",
    "end": "00:09:42.949000",
    "text": "this FS2 we add this term over here F1 y\n0 + y0 so we add to it"
  },
  {
    "start": "00:09:44.519000",
    "end": "00:09:51.350000",
    "text": "another\nblock involving the function"
  },
  {
    "start": "00:09:54.839000",
    "end": "00:09:57.630000",
    "text": "F1 okay so this is basically this inner\nterm over here and then finally we are"
  },
  {
    "start": "00:09:57.640000",
    "end": "00:10:00.190000",
    "text": "term over here and then finally we are\ntaking the F3 of that so this is"
  },
  {
    "start": "00:10:00.200000",
    "end": "00:10:09.110000",
    "text": "taking the F3 of that so this is\nbasically my F3 so if I am a um"
  },
  {
    "start": "00:10:13.120000",
    "end": "00:10:14.750000",
    "text": "Circle if I may Circle this uh this will\nbe let's say"
  },
  {
    "start": "00:10:14.760000",
    "end": "00:10:19.750000",
    "text": "be let's say\na this a will be available over"
  },
  {
    "start": "00:10:19.760000",
    "end": "00:10:22.269000",
    "text": "a this a will be available over\nhere okay so we have now finished with"
  },
  {
    "start": "00:10:22.279000",
    "end": "00:10:25.350000",
    "text": "here okay so we have now finished with\nthe plotting of the first term and now"
  },
  {
    "start": "00:10:25.360000",
    "end": "00:10:27.509000",
    "text": "the plotting of the first term and now\nlet's look at the second"
  },
  {
    "start": "00:10:27.519000",
    "end": "00:10:32.069000",
    "text": "let's look at the second\nterm so the second term is simply FS2 of"
  },
  {
    "start": "00:10:32.079000",
    "end": "00:10:42.310000",
    "text": "term so the second term is simply FS2 of\ny1 y 0 + y0 so I am going to go again"
  },
  {
    "start": "00:10:54.480000",
    "end": "00:10:56.269000",
    "text": "line so I'm going to just take again\nfub1 of"
  },
  {
    "start": "00:10:56.279000",
    "end": "00:10:59.710000",
    "text": "fub1 of\ny0 + y0"
  },
  {
    "start": "00:10:59.720000",
    "end": "00:11:05.389000",
    "text": "y0 + y0\nand I'm simply going to take the F2 of"
  },
  {
    "start": "00:11:09.800000",
    "end": "00:11:14.150000",
    "text": "that and uh this thing over here\nis the point"
  },
  {
    "start": "00:11:19.079000",
    "end": "00:11:25.150000",
    "text": "B however however uh this so this is\nbasically B let me uh throw it over"
  },
  {
    "start": "00:11:28.680000",
    "end": "00:11:37.710000",
    "text": "here to B now what we have is we simply\nadd another of these"
  },
  {
    "start": "00:11:40.680000",
    "end": "00:11:44.590000",
    "text": "guys we add to\nbe this and I would like to Circle that"
  },
  {
    "start": "00:11:44.600000",
    "end": "00:11:48.190000",
    "text": "be this and I would like to Circle that\nover because we see here three things"
  },
  {
    "start": "00:11:48.200000",
    "end": "00:11:52.910000",
    "text": "over because we see here three things\nbeing involved a b and c so this is"
  },
  {
    "start": "00:11:56.079000",
    "end": "00:12:04.269000",
    "text": "C and then B and\nC are added to a"
  },
  {
    "start": "00:12:07.600000",
    "end": "00:12:10.069000",
    "text": "and this will actually be my\nY3 and now that we have this kind of"
  },
  {
    "start": "00:12:10.079000",
    "end": "00:12:12.710000",
    "text": "Y3 and now that we have this kind of\ndiagram we actually can make some really"
  },
  {
    "start": "00:12:12.720000",
    "end": "00:12:17.470000",
    "text": "diagram we actually can make some really\nnice conclusions out of it uh as you can"
  },
  {
    "start": "00:12:17.480000",
    "end": "00:12:21.030000",
    "text": "nice conclusions out of it uh as you can\nsee uh the gradient in the backward pass"
  },
  {
    "start": "00:12:21.040000",
    "end": "00:12:22.509000",
    "text": "see uh the gradient in the backward pass\nso the point number one I want to"
  },
  {
    "start": "00:12:22.519000",
    "end": "00:12:29.350000",
    "text": "so the point number one I want to\nmention is about the gradient flow"
  },
  {
    "start": "00:12:31.040000",
    "end": "00:12:33.750000",
    "text": "so you can see the gradient flow in the\nbackward pass during back propagation"
  },
  {
    "start": "00:12:33.760000",
    "end": "00:12:38.870000",
    "text": "backward pass during back propagation\nnow has a a diverse set of paths and to"
  },
  {
    "start": "00:12:38.880000",
    "end": "00:12:41.629000",
    "text": "now has a a diverse set of paths and to\nactually flow all the way to the input"
  },
  {
    "start": "00:12:41.639000",
    "end": "00:12:44.670000",
    "text": "actually flow all the way to the input\nlet's say it has this path that simply"
  },
  {
    "start": "00:12:44.680000",
    "end": "00:12:47.550000",
    "text": "let's say it has this path that simply\njust follows all the way to the input y"
  },
  {
    "start": "00:12:47.560000",
    "end": "00:12:50.470000",
    "text": "just follows all the way to the input y\nz the other path that goes"
  },
  {
    "start": "00:12:50.480000",
    "end": "00:12:54.590000",
    "text": "z the other path that goes\nthrough this F1 to go to the input this"
  },
  {
    "start": "00:12:54.600000",
    "end": "00:12:57.389000",
    "text": "through this F1 to go to the input this\npath through F2 and F the concatenation"
  },
  {
    "start": "00:12:57.399000",
    "end": "00:13:00.629000",
    "text": "path through F2 and F the concatenation\nF2 and and fub1 or via this skip"
  },
  {
    "start": "00:13:00.639000",
    "end": "00:13:02.829000",
    "text": "F2 and and fub1 or via this skip\nconnection to go to F0 and so on and so"
  },
  {
    "start": "00:13:02.839000",
    "end": "00:13:08.069000",
    "text": "connection to go to F0 and so on and so\non so what we see here is we have a a"
  },
  {
    "start": "00:13:08.079000",
    "end": "00:13:13.750000",
    "text": "on so what we see here is we have a a\ndiverse uh set of"
  },
  {
    "start": "00:13:20.680000",
    "end": "00:13:23.949000",
    "text": "through uh\neach"
  },
  {
    "start": "00:13:25.959000",
    "end": "00:13:30.430000",
    "text": "goes\nthrough um of uh s of of gates I will"
  },
  {
    "start": "00:13:30.440000",
    "end": "00:13:32.750000",
    "text": "through um of uh s of of gates I will\ncall it because this is what we have"
  },
  {
    "start": "00:13:32.760000",
    "end": "00:13:35.230000",
    "text": "call it because this is what we have\nused as a term from back"
  },
  {
    "start": "00:13:35.240000",
    "end": "00:13:41.310000",
    "text": "used as a term from back\npropagation of"
  },
  {
    "start": "00:13:44.079000",
    "end": "00:13:47.110000",
    "text": "varing uh\ndepth okay for varing depth so that's"
  },
  {
    "start": "00:13:47.120000",
    "end": "00:13:49.389000",
    "text": "depth okay for varing depth so that's\nkind of important earlier what we had"
  },
  {
    "start": "00:13:49.399000",
    "end": "00:13:51.470000",
    "text": "kind of important earlier what we had\nwithout those kind of skip connections"
  },
  {
    "start": "00:13:51.480000",
    "end": "00:13:55.030000",
    "text": "without those kind of skip connections\nwe had simply F1 con with two3 in the"
  },
  {
    "start": "00:13:55.040000",
    "end": "00:13:57.829000",
    "text": "we had simply F1 con with two3 in the\nSo-Cal let's say V architecture so here"
  },
  {
    "start": "00:13:57.839000",
    "end": "00:14:01.710000",
    "text": "So-Cal let's say V architecture so here\nwe had a a back propagation that it was"
  },
  {
    "start": "00:14:01.720000",
    "end": "00:14:04.670000",
    "text": "we had a a back propagation that it was\ninvolving um just a single trajectory a"
  },
  {
    "start": "00:14:04.680000",
    "end": "00:14:06.269000",
    "text": "involving um just a single trajectory a\nsingle path through all all of these"
  },
  {
    "start": "00:14:06.279000",
    "end": "00:14:08.389000",
    "text": "single path through all all of these\nGates and at some point the gradient was"
  },
  {
    "start": "00:14:08.399000",
    "end": "00:14:10.990000",
    "text": "Gates and at some point the gradient was\nactually dying and of course a dying"
  },
  {
    "start": "00:14:11.000000",
    "end": "00:14:13.069000",
    "text": "actually dying and of course a dying\ngradient means that specific"
  },
  {
    "start": "00:14:13.079000",
    "end": "00:14:16.150000",
    "text": "gradient means that specific\nfunctionality of my network uh they are"
  },
  {
    "start": "00:14:16.160000",
    "end": "00:14:18.150000",
    "text": "functionality of my network uh they are\nbeing uh updated the parameters are"
  },
  {
    "start": "00:14:18.160000",
    "end": "00:14:20.590000",
    "text": "being uh updated the parameters are\nbeing updated very very slowly or just"
  },
  {
    "start": "00:14:20.600000",
    "end": "00:14:23.590000",
    "text": "being updated very very slowly or just\nsimply seize to be updated so that's not"
  },
  {
    "start": "00:14:23.600000",
    "end": "00:14:25.509000",
    "text": "simply seize to be updated so that's not\nuh really I mean this is one of the key"
  },
  {
    "start": "00:14:25.519000",
    "end": "00:14:27.829000",
    "text": "uh really I mean this is one of the key\nobservations that led to some kind of"
  },
  {
    "start": "00:14:27.839000",
    "end": "00:14:31.350000",
    "text": "observations that led to some kind of\nleveling of of the performance of these"
  },
  {
    "start": "00:14:31.360000",
    "end": "00:14:34.110000",
    "text": "leveling of of the performance of these\nearlier kind of architectures uh as the"
  },
  {
    "start": "00:14:34.120000",
    "end": "00:14:36.350000",
    "text": "earlier kind of architectures uh as the\nnumber of layers were being added up in"
  },
  {
    "start": "00:14:36.360000",
    "end": "00:14:39.269000",
    "text": "number of layers were being added up in\nthis kind of architecture we are"
  },
  {
    "start": "00:14:39.279000",
    "end": "00:14:40.990000",
    "text": "this kind of architecture we are\neffectively Implement what is actually"
  },
  {
    "start": "00:14:41.000000",
    "end": "00:14:44.030000",
    "text": "effectively Implement what is actually\nknown as Highway networks and those"
  },
  {
    "start": "00:14:44.040000",
    "end": "00:14:46.749000",
    "text": "known as Highway networks and those\nHighway highways that we creating for"
  },
  {
    "start": "00:14:46.759000",
    "end": "00:14:50.110000",
    "text": "Highway highways that we creating for\nthe gradient uh empirically has shown"
  },
  {
    "start": "00:14:50.120000",
    "end": "00:14:51.829000",
    "text": "the gradient uh empirically has shown\nthat we are actually can go much much"
  },
  {
    "start": "00:14:51.839000",
    "end": "00:14:55.230000",
    "text": "that we are actually can go much much\ndeeper so we'll see now some depths uh"
  },
  {
    "start": "00:14:55.240000",
    "end": "00:14:57.430000",
    "text": "deeper so we'll see now some depths uh\nuh typical depths that we experience in"
  },
  {
    "start": "00:14:57.440000",
    "end": "00:14:59.629000",
    "text": "uh typical depths that we experience in\nuh we we have in a resent architect in a"
  },
  {
    "start": "00:14:59.639000",
    "end": "00:15:01.509000",
    "text": "uh we we have in a resent architect in a\nmoment the"
  },
  {
    "start": "00:15:01.519000",
    "end": "00:15:04.870000",
    "text": "moment the\nsecond uh aspect of that is a bit more"
  },
  {
    "start": "00:15:04.880000",
    "end": "00:15:06.990000",
    "text": "second uh aspect of that is a bit more\nnuanced and it has to do with what we"
  },
  {
    "start": "00:15:07.000000",
    "end": "00:15:14.509000",
    "text": "nuanced and it has to do with what we\ncall an ensample"
  },
  {
    "start": "00:15:16.759000",
    "end": "00:15:19.590000",
    "text": "learning so in this kind of Ensemble\nlearning architecture what we see is we"
  },
  {
    "start": "00:15:19.600000",
    "end": "00:15:23.310000",
    "text": "learning architecture what we see is we\nsee U uh the concatenation the the"
  },
  {
    "start": "00:15:23.320000",
    "end": "00:15:26.189000",
    "text": "see U uh the concatenation the the\ncombination of three predictors here"
  },
  {
    "start": "00:15:26.199000",
    "end": "00:15:28.189000",
    "text": "combination of three predictors here\nthree main prediction architectures each"
  },
  {
    "start": "00:15:28.199000",
    "end": "00:15:31.069000",
    "text": "three main prediction architectures each\nof those predictors has a"
  },
  {
    "start": "00:15:31.079000",
    "end": "00:15:34.430000",
    "text": "of those predictors has a\nvarying uh kind of functionality so we"
  },
  {
    "start": "00:15:34.440000",
    "end": "00:15:38.269000",
    "text": "varying uh kind of functionality so we\nsee a fairly involved predictor which we"
  },
  {
    "start": "00:15:38.279000",
    "end": "00:15:40.749000",
    "text": "see a fairly involved predictor which we\ncall a we see another predictor which"
  },
  {
    "start": "00:15:40.759000",
    "end": "00:15:43.030000",
    "text": "call a we see another predictor which\ncall B and another predictor that we"
  },
  {
    "start": "00:15:43.040000",
    "end": "00:15:46.550000",
    "text": "call B and another predictor that we\ncall C and what we see at the output are"
  },
  {
    "start": "00:15:46.560000",
    "end": "00:15:50.309000",
    "text": "call C and what we see at the output are\nthe kind of combination of of those"
  },
  {
    "start": "00:15:50.319000",
    "end": "00:15:53.550000",
    "text": "the kind of combination of of those\nsimply um I mean if you are familiar"
  },
  {
    "start": "00:15:53.560000",
    "end": "00:15:55.949000",
    "text": "simply um I mean if you are familiar\nwith ample kind of methods which I'll"
  },
  {
    "start": "00:15:55.959000",
    "end": "00:15:57.309000",
    "text": "with ample kind of methods which I'll\nprovide some kind of background in a"
  },
  {
    "start": "00:15:57.319000",
    "end": "00:16:00.509000",
    "text": "provide some kind of background in a\nmoment we are adding the individual"
  },
  {
    "start": "00:16:00.519000",
    "end": "00:16:02.230000",
    "text": "moment we are adding the individual\nprediction uh"
  },
  {
    "start": "00:16:02.240000",
    "end": "00:16:06.790000",
    "text": "prediction uh\nresults given the input y z uh at the to"
  },
  {
    "start": "00:16:06.800000",
    "end": "00:16:09.230000",
    "text": "results given the input y z uh at the to\nto obtain our final prediction output"
  },
  {
    "start": "00:16:09.240000",
    "end": "00:16:12.749000",
    "text": "to obtain our final prediction output\nthe the so-called Y3 hat okay and and so"
  },
  {
    "start": "00:16:12.759000",
    "end": "00:16:13.870000",
    "text": "the the so-called Y3 hat okay and and so\nthat's"
  },
  {
    "start": "00:16:13.880000",
    "end": "00:16:19.590000",
    "text": "that's\num uh um ample methods have uh proven in"
  },
  {
    "start": "00:16:19.600000",
    "end": "00:16:22.269000",
    "text": "um uh um ample methods have uh proven in\nthe field to be a very powerful uh"
  },
  {
    "start": "00:16:22.279000",
    "end": "00:16:25.309000",
    "text": "the field to be a very powerful uh\napproach uh in solving um you know"
  },
  {
    "start": "00:16:25.319000",
    "end": "00:16:28.710000",
    "text": "approach uh in solving um you know\ncomplex kind of tasks and uh in fact"
  },
  {
    "start": "00:16:28.720000",
    "end": "00:16:30.749000",
    "text": "complex kind of tasks and uh in fact\nsome methods are being used both for"
  },
  {
    "start": "00:16:30.759000",
    "end": "00:16:33.470000",
    "text": "some methods are being used both for\nstructure and unstructured data and in"
  },
  {
    "start": "00:16:33.480000",
    "end": "00:16:35.749000",
    "text": "structure and unstructured data and in\nthe structure kind of data field you"
  },
  {
    "start": "00:16:35.759000",
    "end": "00:16:38.150000",
    "text": "the structure kind of data field you\nhave methods such as gradient boosting"
  },
  {
    "start": "00:16:38.160000",
    "end": "00:16:40.309000",
    "text": "have methods such as gradient boosting\nand so on providing some real"
  },
  {
    "start": "00:16:40.319000",
    "end": "00:16:43.710000",
    "text": "and so on providing some real\nstate-of-the-art results today so a few"
  },
  {
    "start": "00:16:43.720000",
    "end": "00:16:46.710000",
    "text": "state-of-the-art results today so a few\nwords about ample methods is arguably a"
  },
  {
    "start": "00:16:46.720000",
    "end": "00:16:48.710000",
    "text": "words about ample methods is arguably a\nparenthesis uh but I think it's a kind"
  },
  {
    "start": "00:16:48.720000",
    "end": "00:16:49.910000",
    "text": "parenthesis uh but I think it's a kind\nof"
  },
  {
    "start": "00:16:49.920000",
    "end": "00:16:52.389000",
    "text": "of\nworthwhile uh sort of discussing a"
  },
  {
    "start": "00:16:52.399000",
    "end": "00:17:00.350000",
    "text": "worthwhile uh sort of discussing a\nlittle bit about ensample methods"
  },
  {
    "start": "00:17:02.720000",
    "end": "00:17:05.549000",
    "text": "so there are various asample methods but\nI think a common denominator for many of"
  },
  {
    "start": "00:17:05.559000",
    "end": "00:17:10.110000",
    "text": "I think a common denominator for many of\nthem is that the um prediction why hat"
  },
  {
    "start": "00:17:10.120000",
    "end": "00:17:12.870000",
    "text": "them is that the um prediction why hat\nthat we get from the so-called ensample"
  },
  {
    "start": "00:17:12.880000",
    "end": "00:17:21.230000",
    "text": "that we get from the so-called ensample\nalso known as committee"
  },
  {
    "start": "00:17:24.120000",
    "end": "00:17:27.110000",
    "text": "methods where we for form a committee a\ngroup of experts or weak Learners as we"
  },
  {
    "start": "00:17:27.120000",
    "end": "00:17:30.590000",
    "text": "group of experts or weak Learners as we\ncall them um is uh let me call it why"
  },
  {
    "start": "00:17:30.600000",
    "end": "00:17:35.070000",
    "text": "call them um is uh let me call it why\nhat committee is simply uh the average"
  },
  {
    "start": "00:17:35.080000",
    "end": "00:17:38.070000",
    "text": "hat committee is simply uh the average\nlet me call it 1 / capital K of the"
  },
  {
    "start": "00:17:38.080000",
    "end": "00:17:40.669000",
    "text": "let me call it 1 / capital K of the\nsummation from small letter K is equal"
  },
  {
    "start": "00:17:40.679000",
    "end": "00:17:43.990000",
    "text": "summation from small letter K is equal\nto 1 to capital K where capital K is the"
  },
  {
    "start": "00:17:44.000000",
    "end": "00:17:46.789000",
    "text": "to 1 to capital K where capital K is the\nnumber of predictors that we have here"
  },
  {
    "start": "00:17:46.799000",
    "end": "00:17:48.510000",
    "text": "number of predictors that we have here\nwe had three in the rest net"
  },
  {
    "start": "00:17:48.520000",
    "end": "00:17:51.789000",
    "text": "we had three in the rest net\narchitecture of Y hat subk so the"
  },
  {
    "start": "00:17:51.799000",
    "end": "00:17:54.190000",
    "text": "architecture of Y hat subk so the\npremise of emble method is that we don't"
  },
  {
    "start": "00:17:54.200000",
    "end": "00:17:56.390000",
    "text": "premise of emble method is that we don't\nnecessarily to have the single server"
  },
  {
    "start": "00:17:56.400000",
    "end": "00:17:58.950000",
    "text": "necessarily to have the single server\nbullet uh that will solve the very"
  },
  {
    "start": "00:17:58.960000",
    "end": "00:18:01.789000",
    "text": "bullet uh that will solve the very\ncomplicated kind of task of us uh that"
  },
  {
    "start": "00:18:01.799000",
    "end": "00:18:05.110000",
    "text": "complicated kind of task of us uh that\nwe have in front of us but uh a number"
  },
  {
    "start": "00:18:05.120000",
    "end": "00:18:12.430000",
    "text": "we have in front of us but uh a number\nof what we call the so-called weak"
  },
  {
    "start": "00:18:16.320000",
    "end": "00:18:19.149000",
    "text": "predictors uh that it will U not perform\nindividually very well but on aggregate"
  },
  {
    "start": "00:18:19.159000",
    "end": "00:18:21.270000",
    "text": "individually very well but on aggregate\nthey will actually perform much"
  },
  {
    "start": "00:18:21.280000",
    "end": "00:18:23.430000",
    "text": "they will actually perform much\nbetter that and that is really the"
  },
  {
    "start": "00:18:23.440000",
    "end": "00:18:26.950000",
    "text": "better that and that is really the\npremise of that and you know one uh"
  },
  {
    "start": "00:18:26.960000",
    "end": "00:18:29.549000",
    "text": "premise of that and you know one uh\nparallel uh architecture with have in"
  },
  {
    "start": "00:18:29.559000",
    "end": "00:18:32.190000",
    "text": "parallel uh architecture with have in\nthe earlier method uh that we call the"
  },
  {
    "start": "00:18:32.200000",
    "end": "00:18:34.430000",
    "text": "the earlier method uh that we call the\nrest net is kind of resembles that kind"
  },
  {
    "start": "00:18:34.440000",
    "end": "00:18:36.510000",
    "text": "rest net is kind of resembles that kind\nof architecture because we have some"
  },
  {
    "start": "00:18:36.520000",
    "end": "00:18:38.990000",
    "text": "of architecture because we have some\nkind of a summation combination of these"
  },
  {
    "start": "00:18:39.000000",
    "end": "00:18:41.510000",
    "text": "kind of a summation combination of these\nweak predictions the so-called abs and"
  },
  {
    "start": "00:18:41.520000",
    "end": "00:18:44.909000",
    "text": "weak predictions the so-called abs and\nC's that I have explained earlier uh so"
  },
  {
    "start": "00:18:44.919000",
    "end": "00:18:48.110000",
    "text": "C's that I have explained earlier uh so\nthe um uh in Sample methods in general"
  },
  {
    "start": "00:18:48.120000",
    "end": "00:18:50.669000",
    "text": "the um uh in Sample methods in general\nwe have we can consider performance wise"
  },
  {
    "start": "00:18:50.679000",
    "end": "00:18:53.390000",
    "text": "we have we can consider performance wise\nto consist of uh in somewhere in between"
  },
  {
    "start": "00:18:53.400000",
    "end": "00:18:57.230000",
    "text": "to consist of uh in somewhere in between\ntwo bounds so the lower bound the So-Cal"
  },
  {
    "start": "00:18:57.240000",
    "end": "00:19:00.270000",
    "text": "two bounds so the lower bound the So-Cal\nlower performance"
  },
  {
    "start": "00:19:00.280000",
    "end": "00:19:04.110000",
    "text": "lower performance\nbound is obtained uh evidently when"
  },
  {
    "start": "00:19:04.120000",
    "end": "00:19:07.149000",
    "text": "bound is obtained uh evidently when\nyou're you have very correlated"
  },
  {
    "start": "00:19:07.159000",
    "end": "00:19:10.710000",
    "text": "you're you have very correlated\npredictions and if you are not able to"
  },
  {
    "start": "00:19:10.720000",
    "end": "00:19:12.750000",
    "text": "predictions and if you are not able to\nrandomize the operation of each of these"
  },
  {
    "start": "00:19:12.760000",
    "end": "00:19:16.549000",
    "text": "randomize the operation of each of these\npredictors somehow uh we are going to"
  },
  {
    "start": "00:19:16.559000",
    "end": "00:19:18.909000",
    "text": "predictors somehow uh we are going to\nexhibit this kind of lower bound where"
  },
  {
    "start": "00:19:18.919000",
    "end": "00:19:21.270000",
    "text": "exhibit this kind of lower bound where\neither you form a committee or not you"
  },
  {
    "start": "00:19:21.280000",
    "end": "00:19:23.590000",
    "text": "either you form a committee or not you\nget exactly the same performance it's"
  },
  {
    "start": "00:19:23.600000",
    "end": "00:19:26.430000",
    "text": "get exactly the same performance it's\njust like the analogy or the equivalent"
  },
  {
    "start": "00:19:26.440000",
    "end": "00:19:28.710000",
    "text": "just like the analogy or the equivalent\nanalogy I would like to uh sort of share"
  },
  {
    "start": "00:19:28.720000",
    "end": "00:19:31.070000",
    "text": "analogy I would like to uh sort of share\nsh is uh you have a committee of let's"
  },
  {
    "start": "00:19:31.080000",
    "end": "00:19:33.990000",
    "text": "sh is uh you have a committee of let's\nsay a human uh Committee of human"
  },
  {
    "start": "00:19:34.000000",
    "end": "00:19:36.510000",
    "text": "say a human uh Committee of human\nexperts but each expert went to exactly"
  },
  {
    "start": "00:19:36.520000",
    "end": "00:19:38.990000",
    "text": "experts but each expert went to exactly\nthe same school studied exactly the same"
  },
  {
    "start": "00:19:39.000000",
    "end": "00:19:42.270000",
    "text": "the same school studied exactly the same\nfield had exactly the same uh University"
  },
  {
    "start": "00:19:42.280000",
    "end": "00:19:44.310000",
    "text": "field had exactly the same uh University\nprofessors and they are actually now"
  },
  {
    "start": "00:19:44.320000",
    "end": "00:19:47.230000",
    "text": "professors and they are actually now\ncalled to solve the problem and guess"
  },
  {
    "start": "00:19:47.240000",
    "end": "00:19:48.630000",
    "text": "called to solve the problem and guess\nwhat each one of them is actually"
  },
  {
    "start": "00:19:48.640000",
    "end": "00:19:50.950000",
    "text": "what each one of them is actually\noffering exactly the same view well"
  },
  {
    "start": "00:19:50.960000",
    "end": "00:19:52.470000",
    "text": "offering exactly the same view well\nthat's basically where the point where"
  },
  {
    "start": "00:19:52.480000",
    "end": "00:19:54.310000",
    "text": "that's basically where the point where\nyou experiencing a lower performance"
  },
  {
    "start": "00:19:54.320000",
    "end": "00:19:56.830000",
    "text": "you experiencing a lower performance\nbound and the upper performance bound is"
  },
  {
    "start": "00:19:56.840000",
    "end": "00:19:59.270000",
    "text": "bound and the upper performance bound is\na bit more nuanced a bit more"
  },
  {
    "start": "00:19:59.280000",
    "end": "00:20:02.190000",
    "text": "a bit more nuanced a bit more\ncomplicated to kind of understand but"
  },
  {
    "start": "00:20:02.200000",
    "end": "00:20:04.110000",
    "text": "complicated to kind of understand but\nthe best"
  },
  {
    "start": "00:20:04.120000",
    "end": "00:20:06.909000",
    "text": "the best\nperformance uh what we can actually"
  },
  {
    "start": "00:20:06.919000",
    "end": "00:20:09.669000",
    "text": "performance uh what we can actually\nget um I think it's better understood"
  },
  {
    "start": "00:20:09.679000",
    "end": "00:20:12.510000",
    "text": "get um I think it's better understood\nwith an analogy um you don't expect"
  },
  {
    "start": "00:20:12.520000",
    "end": "00:20:14.669000",
    "text": "with an analogy um you don't expect\nevery Committee Member to not make"
  },
  {
    "start": "00:20:14.679000",
    "end": "00:20:17.950000",
    "text": "every Committee Member to not make\nmistakes they will make mistakes but uh"
  },
  {
    "start": "00:20:17.960000",
    "end": "00:20:20.630000",
    "text": "mistakes they will make mistakes but uh\nwhat you want to do is to have a"
  },
  {
    "start": "00:20:20.640000",
    "end": "00:20:22.430000",
    "text": "what you want to do is to have a\ncommittee that they don't make the same"
  },
  {
    "start": "00:20:22.440000",
    "end": "00:20:24.990000",
    "text": "committee that they don't make the same\nmistake at the same time so the"
  },
  {
    "start": "00:20:25.000000",
    "end": "00:20:26.230000",
    "text": "mistake at the same time so the\nso-called"
  },
  {
    "start": "00:20:26.240000",
    "end": "00:20:29.430000",
    "text": "so-called\nuncorrelated uh errors are are involved"
  },
  {
    "start": "00:20:29.440000",
    "end": "00:20:32.470000",
    "text": "uncorrelated uh errors are are involved\nin uh sort of show showing some kind of"
  },
  {
    "start": "00:20:32.480000",
    "end": "00:20:34.270000",
    "text": "in uh sort of show showing some kind of\na performance bound that that is kind of"
  },
  {
    "start": "00:20:34.280000",
    "end": "00:20:37.310000",
    "text": "a performance bound that that is kind of\noutside of the scope of this course but"
  },
  {
    "start": "00:20:37.320000",
    "end": "00:20:40.270000",
    "text": "outside of the scope of this course but\nuh I think it's worthwhile um providing"
  },
  {
    "start": "00:20:40.280000",
    "end": "00:20:43.149000",
    "text": "uh I think it's worthwhile um providing\nsome kind of guidance as to where and"
  },
  {
    "start": "00:20:43.159000",
    "end": "00:20:44.909000",
    "text": "some kind of guidance as to where and\nhow we will be able to achieve that"
  },
  {
    "start": "00:20:44.919000",
    "end": "00:20:47.870000",
    "text": "how we will be able to achieve that\nupper bound uh so the the main three"
  },
  {
    "start": "00:20:47.880000",
    "end": "00:20:49.510000",
    "text": "upper bound uh so the the main three\nways that we can achieve this kind of"
  },
  {
    "start": "00:20:49.520000",
    "end": "00:20:51.870000",
    "text": "ways that we can achieve this kind of\nupper bound or try to achieve the best"
  },
  {
    "start": "00:20:51.880000",
    "end": "00:20:53.990000",
    "text": "upper bound or try to achieve the best\npossible performance out of ample"
  },
  {
    "start": "00:20:54.000000",
    "end": "00:20:58.070000",
    "text": "possible performance out of ample\nmethods the first is the data component"
  },
  {
    "start": "00:20:58.080000",
    "end": "00:20:59.950000",
    "text": "methods the first is the data component\ncan we"
  },
  {
    "start": "00:20:59.960000",
    "end": "00:21:04.029000",
    "text": "can we\nprovide in some way uh different data to"
  },
  {
    "start": "00:21:04.039000",
    "end": "00:21:06.950000",
    "text": "provide in some way uh different data to\ndifferent of to the various kind of weak"
  },
  {
    "start": "00:21:06.960000",
    "end": "00:21:09.350000",
    "text": "different of to the various kind of weak\npredictors that we have here so that we"
  },
  {
    "start": "00:21:09.360000",
    "end": "00:21:12.590000",
    "text": "predictors that we have here so that we\ndo not cause um exactly the same"
  },
  {
    "start": "00:21:12.600000",
    "end": "00:21:15.830000",
    "text": "do not cause um exactly the same\nconclusion for each one of them so the"
  },
  {
    "start": "00:21:15.840000",
    "end": "00:21:20.190000",
    "text": "conclusion for each one of them so the\nsecond is to somehow randomize their"
  },
  {
    "start": "00:21:20.200000",
    "end": "00:21:22.110000",
    "text": "second is to somehow randomize their\noperation um"
  },
  {
    "start": "00:21:22.120000",
    "end": "00:21:27.269000",
    "text": "operation um\nso"
  },
  {
    "start": "00:21:30.640000",
    "end": "00:21:32.669000",
    "text": "randomization is um the second kind of\napproach there and I can actually can"
  },
  {
    "start": "00:21:32.679000",
    "end": "00:21:34.230000",
    "text": "approach there and I can actually can\noffer an example of"
  },
  {
    "start": "00:21:34.240000",
    "end": "00:21:36.230000",
    "text": "offer an example of\nrandomization maybe we can actually"
  },
  {
    "start": "00:21:36.240000",
    "end": "00:21:39.630000",
    "text": "randomization maybe we can actually\noffer a different um set of"
  },
  {
    "start": "00:21:39.640000",
    "end": "00:21:42.470000",
    "text": "offer a different um set of\nhyperparameters um sort of picked by"
  },
  {
    "start": "00:21:42.480000",
    "end": "00:21:43.710000",
    "text": "hyperparameters um sort of picked by\nsome kind of"
  },
  {
    "start": "00:21:43.720000",
    "end": "00:21:46.789000",
    "text": "some kind of\ndistribution uh in this uh architecture"
  },
  {
    "start": "00:21:46.799000",
    "end": "00:21:50.909000",
    "text": "distribution uh in this uh architecture\nsee over here uh in in this course u in"
  },
  {
    "start": "00:21:50.919000",
    "end": "00:21:52.750000",
    "text": "see over here uh in in this course u in\nsome other approaches where we have"
  },
  {
    "start": "00:21:52.760000",
    "end": "00:21:55.430000",
    "text": "some other approaches where we have\nlet's say uh decision trees involved in"
  },
  {
    "start": "00:21:55.440000",
    "end": "00:21:57.269000",
    "text": "let's say uh decision trees involved in\nthese predictors again for structure"
  },
  {
    "start": "00:21:57.279000",
    "end": "00:21:59.310000",
    "text": "these predictors again for structure\ndata I'm referring to then we can"
  },
  {
    "start": "00:21:59.320000",
    "end": "00:22:01.669000",
    "text": "data I'm referring to then we can\nrandomize their operation by picking"
  },
  {
    "start": "00:22:01.679000",
    "end": "00:22:04.510000",
    "text": "randomize their operation by picking\ndifferent uh features that they split uh"
  },
  {
    "start": "00:22:04.520000",
    "end": "00:22:07.830000",
    "text": "different uh features that they split uh\ntheir uh sort of trees and there are so"
  },
  {
    "start": "00:22:07.840000",
    "end": "00:22:10.110000",
    "text": "their uh sort of trees and there are so\nmany approaches that are you know I"
  },
  {
    "start": "00:22:10.120000",
    "end": "00:22:12.630000",
    "text": "many approaches that are you know I\nguess too many to to quote over here but"
  },
  {
    "start": "00:22:12.640000",
    "end": "00:22:14.110000",
    "text": "guess too many to to quote over here but\nthe third approach is a bit more"
  },
  {
    "start": "00:22:14.120000",
    "end": "00:22:16.070000",
    "text": "the third approach is a bit more\nrelevant in this specific rest not"
  },
  {
    "start": "00:22:16.080000",
    "end": "00:22:25.470000",
    "text": "relevant in this specific rest not\narchitecture is to Simply use different"
  },
  {
    "start": "00:22:34.159000",
    "end": "00:22:37.430000",
    "text": "so the weak Learners that we called over\nhere are uh involved in the rest net"
  },
  {
    "start": "00:22:37.440000",
    "end": "00:22:41.430000",
    "text": "here are uh involved in the rest net\narchitecture and uh so here we have a"
  },
  {
    "start": "00:22:41.440000",
    "end": "00:22:44.029000",
    "text": "architecture and uh so here we have a\npredictor of some complexity we have"
  },
  {
    "start": "00:22:44.039000",
    "end": "00:22:46.190000",
    "text": "predictor of some complexity we have\nhere another"
  },
  {
    "start": "00:22:46.200000",
    "end": "00:22:49.110000",
    "text": "here another\npredictor larger complexity and yet"
  },
  {
    "start": "00:22:49.120000",
    "end": "00:22:51.549000",
    "text": "predictor larger complexity and yet\nanother predictor or even larger"
  },
  {
    "start": "00:22:51.559000",
    "end": "00:22:54.350000",
    "text": "another predictor or even larger\ncomplexity we have effectively"
  },
  {
    "start": "00:22:54.360000",
    "end": "00:22:56.710000",
    "text": "complexity we have effectively\nimplementing um you know the third"
  },
  {
    "start": "00:22:56.720000",
    "end": "00:22:59.190000",
    "text": "implementing um you know the third\napproach where we have um"
  },
  {
    "start": "00:22:59.200000",
    "end": "00:23:01.750000",
    "text": "approach where we have um\nthose different week Learners each one"
  },
  {
    "start": "00:23:01.760000",
    "end": "00:23:04.950000",
    "text": "those different week Learners each one\nis offering a view and then uh finally"
  },
  {
    "start": "00:23:04.960000",
    "end": "00:23:06.950000",
    "text": "is offering a view and then uh finally\nthe network is deciding based on the"
  },
  {
    "start": "00:23:06.960000",
    "end": "00:23:10.070000",
    "text": "the network is deciding based on the\ncomposition of those views okay so that"
  },
  {
    "start": "00:23:10.080000",
    "end": "00:23:13.630000",
    "text": "composition of those views okay so that\nhas been shown to uh sort of provide"
  },
  {
    "start": "00:23:13.640000",
    "end": "00:23:15.669000",
    "text": "has been shown to uh sort of provide\nperformance advantages and and that's"
  },
  {
    "start": "00:23:15.679000",
    "end": "00:23:17.870000",
    "text": "performance advantages and and that's\nwhat we kind of had this discussion"
  },
  {
    "start": "00:23:17.880000",
    "end": "00:23:20.630000",
    "text": "what we kind of had this discussion\nabout and Sample methods and the third"
  },
  {
    "start": "00:23:20.640000",
    "end": "00:23:23.070000",
    "text": "about and Sample methods and the third\nuh kind of Advantage I wanted to quote"
  },
  {
    "start": "00:23:23.080000",
    "end": "00:23:29.590000",
    "text": "uh kind of Advantage I wanted to quote\nhere for rest n is there scalability"
  },
  {
    "start": "00:23:30.760000",
    "end": "00:23:33.149000",
    "text": "so the\nscalability um should be understood from"
  },
  {
    "start": "00:23:33.159000",
    "end": "00:23:35.669000",
    "text": "scalability um should be understood from\nthe point of view of uh"
  },
  {
    "start": "00:23:35.679000",
    "end": "00:23:39.669000",
    "text": "the point of view of uh\ncomplexity um we are effectively able to"
  },
  {
    "start": "00:23:39.679000",
    "end": "00:23:42.870000",
    "text": "complexity um we are effectively able to\nhave three six n or whatever number of"
  },
  {
    "start": "00:23:42.880000",
    "end": "00:23:45.029000",
    "text": "have three six n or whatever number of\nuh residual blocks each one of them will"
  },
  {
    "start": "00:23:45.039000",
    "end": "00:23:49.870000",
    "text": "uh residual blocks each one of them will\nactually be uh exactly the same um as uh"
  },
  {
    "start": "00:23:49.880000",
    "end": "00:23:52.269000",
    "text": "actually be uh exactly the same um as uh\nyou know any other block over here and"
  },
  {
    "start": "00:23:52.279000",
    "end": "00:23:54.390000",
    "text": "you know any other block over here and\ntherefore we are able to"
  },
  {
    "start": "00:23:54.400000",
    "end": "00:23:55.990000",
    "text": "therefore we are able to\naccommodate"
  },
  {
    "start": "00:23:56.000000",
    "end": "00:23:58.950000",
    "text": "accommodate\narchitectures that are have various"
  },
  {
    "start": "00:23:58.960000",
    "end": "00:24:01.750000",
    "text": "architectures that are have various\nvarious number of these blocks let's say"
  },
  {
    "start": "00:24:01.760000",
    "end": "00:24:06.470000",
    "text": "various number of these blocks let's say\nwe see resets with 18 layers 34 layers"
  },
  {
    "start": "00:24:06.480000",
    "end": "00:24:12.909000",
    "text": "we see resets with 18 layers 34 layers\nyou know 50 layers 150 uh 102 layers"
  },
  {
    "start": "00:24:12.919000",
    "end": "00:24:16.149000",
    "text": "you know 50 layers 150 uh 102 layers\neven 150 layers these are the numbers"
  },
  {
    "start": "00:24:16.159000",
    "end": "00:24:18.789000",
    "text": "even 150 layers these are the numbers\nthat we have uh we have defined already"
  },
  {
    "start": "00:24:18.799000",
    "end": "00:24:20.549000",
    "text": "that we have uh we have defined already\nexisting architectures and this is kind"
  },
  {
    "start": "00:24:20.559000",
    "end": "00:24:23.269000",
    "text": "existing architectures and this is kind\nof important when you have perception"
  },
  {
    "start": "00:24:23.279000",
    "end": "00:24:25.870000",
    "text": "of important when you have perception\nsystems that need to comply to some"
  },
  {
    "start": "00:24:25.880000",
    "end": "00:24:28.149000",
    "text": "systems that need to comply to some\nrealtime latency requirement evidently"
  },
  {
    "start": "00:24:28.159000",
    "end": "00:24:30.470000",
    "text": "realtime latency requirement evidently\nthe larger the number of layers you have"
  },
  {
    "start": "00:24:30.480000",
    "end": "00:24:33.470000",
    "text": "the larger the number of layers you have\nthe longer the latencies that you are"
  },
  {
    "start": "00:24:33.480000",
    "end": "00:24:36.590000",
    "text": "the longer the latencies that you are\ngoing to experience taking an image"
  },
  {
    "start": "00:24:36.600000",
    "end": "00:24:40.149000",
    "text": "going to experience taking an image\nthrough this kind of pipeline so if we"
  },
  {
    "start": "00:24:40.159000",
    "end": "00:24:43.750000",
    "text": "through this kind of pipeline so if we\num have let's say a latency of let's say"
  },
  {
    "start": "00:24:43.760000",
    "end": "00:24:44.750000",
    "text": "um have let's say a latency of let's say\n80"
  },
  {
    "start": "00:24:44.760000",
    "end": "00:24:48.430000",
    "text": "80\nMCS uh we can um and and therefore we"
  },
  {
    "start": "00:24:48.440000",
    "end": "00:24:51.630000",
    "text": "MCS uh we can um and and therefore we\nare not able to accommodate 102 uh 102"
  },
  {
    "start": "00:24:51.640000",
    "end": "00:24:53.269000",
    "text": "are not able to accommodate 102 uh 102\nlayers where definitely going to be"
  },
  {
    "start": "00:24:53.279000",
    "end": "00:24:56.190000",
    "text": "layers where definitely going to be\naccommodating let's say 50 layers and"
  },
  {
    "start": "00:24:56.200000",
    "end": "00:24:58.830000",
    "text": "accommodating let's say 50 layers and\nthe exactly the same technology exactly"
  },
  {
    "start": "00:24:58.840000",
    "end": "00:25:02.430000",
    "text": "the exactly the same technology exactly\nthe same uh thinking um and behavior of"
  },
  {
    "start": "00:25:02.440000",
    "end": "00:25:06.430000",
    "text": "the same uh thinking um and behavior of\nrest Nets will uh be in either of the uh"
  },
  {
    "start": "00:25:06.440000",
    "end": "00:25:08.149000",
    "text": "rest Nets will uh be in either of the uh\nnumbers quoted here in terms of number"
  },
  {
    "start": "00:25:08.159000",
    "end": "00:25:09.909000",
    "text": "numbers quoted here in terms of number\nof layers so all of these three"
  },
  {
    "start": "00:25:09.919000",
    "end": "00:25:12.470000",
    "text": "of layers so all of these three\nadvantages are coming together to"
  },
  {
    "start": "00:25:12.480000",
    "end": "00:25:14.669000",
    "text": "advantages are coming together to\nprovide a fairly robust architecture has"
  },
  {
    "start": "00:25:14.679000",
    "end": "00:25:17.190000",
    "text": "provide a fairly robust architecture has\nactually proven in the field in both"
  },
  {
    "start": "00:25:17.200000",
    "end": "00:25:19.269000",
    "text": "actually proven in the field in both\nreal time and unreal time applications"
  },
  {
    "start": "00:25:19.279000",
    "end": "00:25:23.190000",
    "text": "real time and unreal time applications\nand able to uh extract uh features"
  },
  {
    "start": "00:25:23.200000",
    "end": "00:25:26.310000",
    "text": "and able to uh extract uh features\nprovide if you like representations on"
  },
  {
    "start": "00:25:26.320000",
    "end": "00:25:30.190000",
    "text": "provide if you like representations on\nuh visual uh imagery that we have the"
  },
  {
    "start": "00:25:30.200000",
    "end": "00:25:31.669000",
    "text": "uh visual uh imagery that we have the\nthe imageries that we are feeding into"
  },
  {
    "start": "00:25:31.679000",
    "end": "00:25:34.679000",
    "text": "the imageries that we are feeding into\nthem"
  }
]