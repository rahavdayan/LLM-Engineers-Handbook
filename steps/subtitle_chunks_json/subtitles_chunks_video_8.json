[
  {
    "start": "00:00:00.000000",
    "end": "00:00:29.320000",
    "text": "we are in the trajectory where we are going to obtain a blog diagram of our first classifier binary classifier that is but before we do so i think its worthwhile kindof thinking  a bit about the two general frameworks which are present in the design of this probabilistic models that will give us the functional form of this binary classifier so in general there are two frameworks"
  },
  {
    "start": "00:00:29.320000",
    "end": "00:00:43.430000",
    "text": "the firstone both of these frameworks involve the posterior probability but in a kind of a different way so the first framework iscalliscriminative and the second one is calledgenerate and the difference between the two is quite important and they are although they both effectively are modeling in a different way at the posterior let me call this posterior"
  },
  {
    "start": "00:00:43.430000",
    "end": "00:01:25.680000",
    "text": "y of the class small letter k givenx which as we have seen in the probability review section this posterior is the x givenykp of york divided by p ofq so this is the posterior and these two frameworks as we will see kind of model it a bit differently so thatwill be discussing in extensively"
  },
  {
    "start": "00:01:25.680000",
    "end": "00:01:43.680000",
    "text": "in this binary classifier block diagram is the one that will involve direct modeling of the posterior so the discriminative i call it the first framework and the second framework sothe first framework somethods model theso we get this posterior from the block diagram itself"
  },
  {
    "start": "00:01:43.680000",
    "end": "00:01:50.240000",
    "text": "and while the generative ones are effectively model the posterior in parts in its from each kind of components so well first deal with the socalled discriminative classifiers and i want to connect the earlier discussion we had about the radar problem in that kind of video we have introduced a problem where we had we actually went and drew all the areas under the two probability distributions that were given raise to the probability of mistake so i want to flip the coin now and actually h sortof discuss the probability of being correct not the misclassification error but the u"
  },
  {
    "start": "00:01:50.240000",
    "end": "00:02:46.680000",
    "text": "when we when we are the socalled true positive events and trying to maximize them instead of trying to minimize them is classification error soi want to kind of come up with some reasonably intuitive answer to the following question which im writing over here"
  },
  {
    "start": "00:02:46.680000",
    "end": "00:02:55.550000",
    "text": "why theposterior pofyk given xandmetrix so if i kinda repeat this kind of discussion but from as i said from the probability of being correct in this kind of classification problem we have again this kind of two integrals but these two integrals now capture the correct events this is when we have probability of y is equal to 0"
  },
  {
    "start": "00:02:55.550000",
    "end": "00:03:00.239000",
    "text": "dx plus another integral r1 probability ofthe flipped areas that from the ones we have actually drew if you want going ahead and review that kind of video that will actually be helpful"
  },
  {
    "start": "00:03:00.239000",
    "end": "00:03:07.560000",
    "text": "which is of course equal to a summation in general for this is now the general case where we have capital one to capital k in this specific capital q is equal to two but this formula that im actually writing here is going to be general for capital k classes or of the integral over the regions rk of the"
  },
  {
    "start": "00:03:07.560000",
    "end": "00:03:20.120000",
    "text": "so im replacing the joint with the posterior timesmarginal and now its actually a bit more evident how maximizing pcorrect effectively involves maximizing the posterior because this term over here isand independent of thessignment of q to the label k so now we have connected the direct connection effectively that of the probability of being correct and the maximization"
  },
  {
    "start": "00:03:20.120000",
    "end": "00:03:42.120000",
    "text": "trying to maximize the probability of being correct effectively means maximizing the posterior probability so let me write that down because its kind of important maximizing pcorrective correct is equivalent to maximizing p of yuk y yuk given z effectively this points to the following if we are to plot the properity distribution of the posterior actually we will see here well see something like that lets plot the posterior probilitydistribution we went from"
  },
  {
    "start": "00:03:42.120000",
    "end": "00:04:00.439000",
    "text": "we went from distributions at such as this if you remember back in the u in the discussion of the binaryclassifier something like that we have seen of q and this is the probability of xcomma y this was the probability of probability of x comma y isal to 1 and if we are to plot the posterior probability distribution we will be coming up with something that it will look like this"
  },
  {
    "start": "00:04:00.439000",
    "end": "00:04:30.520000",
    "text": "in general this is  a very general kind ofplot so this is one to make sure that we do not exceed the one probability of one so this is the probability of y is equal to 0 givenx and this is the probability of iel1 givenx and so die histoograms that our have actuallyis coming to us as as a lets say a new x that we would like to classify as positive or negative"
  },
  {
    "start": "00:04:30.520000",
    "end": "00:04:32.880000",
    "text": "let me say x new that we have never seen before touches these two curves in this kind of two points i actually can actually see here that these two points correspond tothe discrete so this is this is the for the zeroth class and this is for the lets say class one this is the probability mass function of the posterior distribution at the output of our predictor"
  },
  {
    "start": "00:04:32.880000",
    "end": "00:04:46.880000",
    "text": "so this is the p of z is equal 0 given qq as partnew and this the probability of y is equal to 1 given xnew and so what we have just recognized over here is that all we have to do is always pick the pro the class that gives us the maximum posterior probability output and rest assured if we do that we are maximizing the probability of being correct"
  },
  {
    "start": "00:04:46.880000",
    "end": "00:04:54.510000",
    "text": "so this discussion kindof resulted into this kind of intuitive conclusion but it was not really evident initially how the posters and the probability of being correct arelated so continuing now for the discussion we just had on thediscriminative kind of classifiers i directly model the predict the posterior probability we can actually write the posterior probability as follows"
  },
  {
    "start": "00:04:54.510000",
    "end": "00:05:22.670000",
    "text": "the y lets say is equal to one given x is the probability of x given y is to 1 time the probability of   y is to 1 divided by the probability of that we have reviewed plus the probability of x giveny is to 1 the probability of y is equal1 so if you divide both terms by the if we divide both terms with with a pro with the numerator we will come up with the following expression 1 1 the probabilityof x giveny is equal to 1 probability of given that probability of  0 probability of    is equal to 0"
  },
  {
    "start": "00:05:22.670000",
    "end": "00:05:44.469000",
    "text": "to the minus1 andthis is now related to the probability of odds because the not the probability was the chanceswe now write down the odds is the ratiof of this divided by of divided by this so for example in a horse race where we have  a horse that runs 100 races and wins 25 times and loses the other 75 times the probability of winning is 25 over 100 thats well known to us 25 but the odds are 25 over 75 or 33years or one win to three losses"
  },
  {
    "start": "00:05:44.469000",
    "end": "00:05:57.240000",
    "text": "so this is what we have actually defined over here in terms of our odds the probability of winning to the probability number if we assign model it as a posst number typically we use the e to the power of some kind of positive number a to model that then we can and you know have effectively these two expressions"
  },
  {
    "start": "00:05:57.240000",
    "end": "00:06:43.199000",
    "text": "and from these two expressions we can actually write now the form of the posterior probability distribution so that is the probability of yu is equal to 1 given x is 11 cus a and this is  a wellknown function that is actually called the sizemem functionbecause it is when we actually plot this function"
  },
  {
    "start": "00:06:43.199000",
    "end": "00:07:19.360000",
    "text": "it will look something likethat so over here will be 05 and over here will be theone and it will look like a sigmoid that will give will take as input a and will provide sigma of a and all the output is going to be constrained between 0 and one"
  },
  {
    "start": "00:07:19.360000",
    "end": "00:07:31.680000",
    "text": "so we have effectively came up with this kindof expression of the sort of  a pro posterior probability distribution at the output of  a sigmoidal unit with having as argument some kind of input a now if and this is kind of motivates the kind of logistic regression if a is a linear combination is theone orcombination ofeatures"
  },
  {
    "start": "00:07:31.680000",
    "end": "00:07:40.599000",
    "text": "let say a is w transposing f of x we have seen both of them notations in our linear regression kindof example then this model of theposterior is called logisticregression which is  a wellknown and fairly popular way to do binary classification so effectively over here"
  },
  {
    "start": "00:07:40.599000",
    "end": "00:07:51.469000",
    "text": "we have the assigned the to the logs so another way actually of seeing it is that if we take here the log of thetheoddas in other words the log of theprobability probability of x commay is 1 divided by the probability ofxcomma y is equal to 0 so this is going to be effectively a and if this a is equal to w transpose f of x this is the form of u logistic regression so the logistic regression is followed by the is actually implemented using the following diagram as its actually indicated here"
  },
  {
    "start": "00:07:51.469000",
    "end": "00:08:04.520000",
    "text": "first form  a linear combination of features and this is the dot product in other words between the feature vector and the parameters of the of our model w and then pass that through symbolid  commas i  w so it is being effectively a posterior probability at theoutput so we kind of obtain the logistic regression kindofa first principle so the block diagram is going to be w transpose f of x where we have taken x and very similar to what we have seen in logistic regression"
  },
  {
    "start": "00:08:04.520000",
    "end": "00:00:-1.000000",
    "text": "we went through a featu riser to obtain i ofy which we have used in this kind of dot product to form scalar a and this scalar a is at the input of sigma and that y hat rest assur is going to be first classifier"
  },
  {
    "start": "00:00:-1.000000",
    "end": "00:08:04.520000",
    "text": "that we will be calling ageneralized linear model and we call it generalized because of the nonlinear unit which is definitely nonlinear because the every smooidal unit can take any number from minus one million lets say to plus one million but it compresses that into  a dynamic r between 0 and 1 we definitely want the output to be 0 and 1"
  },
  {
    "start": "00:08:04.520000",
    "end": "00:10:25.790000",
    "text": "because we have interpreted the output as  a pro as  a posterior probability but definitely there from a to the posterior and thats why we call it generalized but definitely its a linear model in  a sense that it is you know one of the blocks of the diagram involves  a linear unit a linear combination of the features fxso all we need to do now is to attach to it two things"
  },
  {
    "start": "00:10:25.790000",
    "end": "00:00:-1.000000",
    "text": "the first is the binary crossentropy loss which will accept also the ground truth and this binary cross entropy loss will befeeding the well known to us stochastic graded descent kind and thats why i call it generalized but definitelyupdate will feed the w and will update the w at every iteration so this block diagram is no surprise to us"
  },
  {
    "start": "00:00:-1.000000",
    "end": "00:00:-1.000000",
    "text": "by now we have seen it so many times in both linear regression and now classification and the only thing that remains to be done over here is to come up with a expression of the crossentropy loss the binary crossentropy loss with respect to the set of parameters w and this can be shown to be u some form such as thisal to 1 to m of yus i or pi of q then"
  },
  {
    "start": "00:00:-1.000000",
    "end": "00:10:25.790000",
    "text": "for us to implement sastic grade descent and now we will see in a notebook how the stoas r descent is powering logistic regression and in fact its al also worthwhile commenting on how the name kind of logistic regression came to be attached to this kindof block diagram"
  },
  {
    "start": "00:10:25.790000",
    "end": "00:13:26.959000",
    "text": "and in fact if we treat this binary classification problem like  a regression problem and we plot over here the u socalled xis versus the yu similarly what we have seen in so many regression problems then definitely ry is discrete randomvariable and take values between zero lets say and one and certainly for in this kind of neighborhood well see many assignments to zero and in this neighborhood remember the radar problem"
  },
  {
    "start": "00:13:26.959000",
    "end": "00:13:44.750000",
    "text": "high signal strength low signal strength high signal strength most often we will get a positive prediction of our attacks and over here were going to have  a negative prediction of our attacks here were actually plotting here the ground truths and if we are to do regression to fit this data in  very similar way as i am we have done earlier"
  },
  {
    "start": "00:13:44.750000",
    "end": "00:13:44.750000",
    "text": "probably will come up with a kind of  straight line that attempts to maximize some objective kindof function so this is straight line regression is not going to be very appropriate for  a classification problem because we are expecting our predictor to produce values always between zero and one so what we do here is we are applying the sigmoidal so this is is effectively the line that generates the a when we have no features no featurization in this specific syle example and so the sixibed we have done earlier probably will come up with a   or evenmatch effectively"
  },
  {
    "start": "00:13:44.750000",
    "end": "00:15:37.269000",
    "text": "the it has a linear component over here and will compress everything between zero and one so thats another way of kindofgraphically remembering logistic regression as an attempt to do regression but at the same time with  a kind of a compressive step okay so now what we will do to conclude a little bit"
  },
  {
    "start": "00:15:37.269000",
    "end": "00:16:16.720000",
    "text": "the topic of classification is to just in passing quote a couple things about the second framework that i have mentioned the soal generative classification frameworks in the generativeclassification framework were again going to be task to calculate the posterior probabilitythat we see here for in general kind of k glasses and thisposterior is going to be evidently given by this general kind oformula so in the generative approach we will do two steps instead of coming up with the block diagram that generates that from directly and models the posterior directly"
  },
  {
    "start": "00:16:16.720000",
    "end": "00:16:32.759000",
    "text": "as we have done with logistic aggression we will first do two steps one is tomarginal and then come to some degree of approximation because its actuallytypically a very expensive for large dimensions for largen this is  a very expensive calculationso for we will typically involve some form of approximation for calculating this denominator over here of thebase is  a famous  generative classification method and we actually going to see that when we come into language modeling and some other tasks later on in some videos"
  },
  {
    "start": "00:16:32.759000",
    "end": "00:18:00.149000",
    "text": "so i will take  rain check to discuss it at that moment and revisit if you like the generative classification framework and the discussion of on bas is not reallyessential right now for us to progress"
  }
]