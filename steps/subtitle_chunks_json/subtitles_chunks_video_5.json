[
  {
    "start": "00:00:00.000000",
    "end": "00:00:08.760000",
    "text": "in an earlier video we saw the structure of the convolutional neuron and how many of these neurons in the form of filters"
  },
  {
    "start": "00:00:08.760000",
    "end": "00:00:21.439000",
    "text": "are you know coming together to form a layer and how the multiple layers are coming again together stacked to implement this binary classification task"
  },
  {
    "start": "00:00:21.439000",
    "end": "00:00:32.399000",
    "text": "in this video that we have looked at on cats versus dogs"
  },
  {
    "start": "00:00:32.399000",
    "end": "00:00:39.470000",
    "text": "now we will be using exactly the same model that we have built for that kind of task and in this specific case what we were interested now to see is to validate what we have said earlier about the some"
  },
  {
    "start": "00:00:39.470000",
    "end": "00:00:47.480000",
    "text": "an earlier video i was looking at on cats vskind of a structure or pattern that we see in the features that the convolution un networks kindofread so"
  },
  {
    "start": "00:00:47.480000",
    "end": "00:00:55.199000",
    "text": "in this notebook borrowed from the book deep learning with python what we actually can see"
  },
  {
    "start": "00:00:55.199000",
    "end": "00:01:03.719000",
    "text": "a couple of things the first is the socalled u the intermediate convolution network outputs"
  },
  {
    "start": "00:01:03.719000",
    "end": "00:01:10.159000",
    "text": "these are effectively what are each layer kind of presents to the layer above it"
  },
  {
    "start": "00:01:10.159000",
    "end": "00:01:15.520000",
    "text": "and i think its worthwhile kind of going through that first"
  },
  {
    "start": "00:01:15.520000",
    "end": "00:01:19.590000",
    "text": "and as we said this is the sort of exactly the same architecture we have seen earlier for"
  },
  {
    "start": "00:01:19.590000",
    "end": "00:01:27.200000",
    "text": "that specific data set and this is the input images"
  },
  {
    "start": "00:01:27.200000",
    "end": "00:01:30.840000",
    "text": "the 100 approximately 150 by 150 pixels and natural reccol images"
  },
  {
    "start": "00:01:30.840000",
    "end": "00:01:38.439000",
    "text": "and this is the you know first layer what it really learns"
  },
  {
    "start": "00:01:38.439000",
    "end": "00:01:56.880000",
    "text": "and as you can see the output of the output kinda feature map it kind of presents an almost identical figure to the sort of input picture input image"
  },
  {
    "start": "00:01:56.880000",
    "end": "00:02:11.790000",
    "text": "except that this image over here emphasizes the edge so these are the as we said the primitive kind of shapes that the first initial layers of the conet are actually learning and we can actually go and lookat each and every layer and i think the over here"
  },
  {
    "start": "00:02:11.790000",
    "end": "00:02:19.760000",
    "text": "i think this"
  },
  {
    "start": "00:02:19.760000",
    "end": "00:02:26.720000",
    "text": "this im figure over here shows what is really happening so the initial layers are learning"
  },
  {
    "start": "00:02:26.720000",
    "end": "00:02:28.920000",
    "text": "i will call it a visual content"
  },
  {
    "start": "00:02:28.920000",
    "end": "00:02:35.280000",
    "text": "the same kind of visual content as our eyes kindofsee in the image"
  },
  {
    "start": "00:02:35.280000",
    "end": "00:02:40.680000",
    "text": "lets say you can very clearly see the shape and form of of the of the cut over here"
  },
  {
    "start": "00:02:40.680000",
    "end": "00:02:53.949000",
    "text": "but as we actually going further deeper into the network then the representations that are actually being created are becoming more abstract to the point where this is the fifth layer as your own"
  },
  {
    "start": "00:02:53.949000",
    "end": "00:02:58.560000",
    "text": "andsee from that point onwards"
  },
  {
    "start": "00:02:58.560000",
    "end": "00:03:07.120000",
    "text": "we still see some of the feature maps that are being created remember the feature maps that are being created are volumes"
  },
  {
    "start": "00:03:07.120000",
    "end": "00:03:12.879000",
    "text": "so what actually we see here are the flattened version of those volumes"
  },
  {
    "start": "00:03:12.879000",
    "end": "00:03:21.599000",
    "text": "so we plot here the special dimensions that are coming at at the output on each image"
  },
  {
    "start": "00:03:21.599000",
    "end": "00:03:36.239000",
    "text": "but we are effectively flattening all of the filters that we have u used all of the depth of the feature maps that we have used to create this volume"
  },
  {
    "start": "00:03:36.239000",
    "end": "00:03:41.550000",
    "text": "so here you have the sixth layer the seventh layersee here"
  },
  {
    "start": "00:03:41.550000",
    "end": "00:03:54.680000",
    "text": "we from the seventh layer onwards we are not really able to see any of the sort of visual characteristics of of a cut"
  },
  {
    "start": "00:03:54.680000",
    "end": "00:03:59.040000",
    "text": "so this become a fairly abstract kind of representation"
  },
  {
    "start": "00:03:59.040000",
    "end": "00:04:10.959000",
    "text": "here you can actually also see very clearly the impact of max pooling and how we can actually start with  a representation"
  },
  {
    "start": "00:04:10.959000",
    "end": "00:04:12.439000",
    "text": "and what is the max pooling operator with cal of 2x two is actually doing"
  },
  {
    "start": "00:04:12.439000",
    "end": "00:04:21.949000",
    "text": "is actually picking at the more essential kind of in features that are presented to it so if you compare this image and these imagesand so thats effectively"
  },
  {
    "start": "00:04:21.949000",
    "end": "00:04:51.440000",
    "text": "at this point we have the representations that are going to be needed after the stochastic rated descent kind of converges and provided we dont have any overfitting and so on these representations are the ones that we are going to be flattening to and then feed them into the fully connected layers that constitute our head"
  },
  {
    "start": "00:04:51.440000",
    "end": "00:05:00.199000",
    "text": "and if everything goes okay this head will actually see and work on those representations to actually do the binary classification"
  },
  {
    "start": "00:05:00.199000",
    "end": "00:05:05.680000",
    "text": "okay so this is what we have seenactually see in the feature maps"
  },
  {
    "start": "00:05:05.680000",
    "end": "00:05:09.560000",
    "text": "and i think its also worthwhile understanding what we actually see now"
  },
  {
    "start": "00:05:09.560000",
    "end": "00:05:18.080000",
    "text": "in as far as the filters what really the contents of those filters are to visualize those filters"
  },
  {
    "start": "00:05:18.080000",
    "end": "00:05:21.919000",
    "text": "what we actually do is we define a specific loss function"
  },
  {
    "start": "00:05:21.919000",
    "end": "00:05:38.520000",
    "text": "the details are kindof outside of this course of the scope of this course but the at high level what we do is we try to find input images that maximize the activations that those filter produce and therefore in the process of doing so we are able to out from thisoptimization process to retrieve those filter values"
  },
  {
    "start": "00:05:38.520000",
    "end": "00:05:51.800000",
    "text": "so here we see just the first 64 filters out of the many more that we have used"
  },
  {
    "start": "00:05:51.800000",
    "end": "00:06:11.560000",
    "text": "i think we used all the way up to 256 filters but we here we see the first 64 filters in a 8by8 kind of pattern and as you can actually see the u this filter over here is for the various kind of layers so these are effectively"
  },
  {
    "start": "00:06:11.560000",
    "end": "00:06:22.479000",
    "text": "we have the layers going from the beginning of this fil of the the beginning of the network all the way to the u towards the just before the head of thisthe last layer just before the head"
  },
  {
    "start": "00:06:22.479000",
    "end": "00:06:48.000000",
    "text": "and as you can actually see here in every layer we learn effectively a collection of these filters that it will decompose the input image the input the input the input the input feature maps are being decomposed"
  },
  {
    "start": "00:06:48.000000",
    "end": "00:07:11.110000",
    "text": "so imagine that you have now not only one filter but you have lets say 64 of them so the each one of those filters is one component out of the lets say 64 that the inputfeature map will be deccomposed so you can read about this as those are the components of that decomposition so this"
  },
  {
    "start": "00:07:11.110000",
    "end": "00:07:28.160000",
    "text": "so in the last kind layer over here we have 64 components for those who have some background on principal component analysis"
  },
  {
    "start": "00:07:28.160000",
    "end": "00:07:41.560000",
    "text": "theres some something to it along those lines but the composition over here is definitely exactly the same as any other decompostion"
  },
  {
    "start": "00:07:41.560000",
    "end": "00:07:50.080000",
    "text": "its just that these components as the layers are becoming deeper and deeper"
  },
  {
    "start": "00:07:50.080000",
    "end": "00:07:52.710000",
    "text": "are these components are"
  },
  {
    "start": "00:07:52.710000",
    "end": "00:07:52.720000",
    "text": "are you know look quite different and this as you cansee here"
  },
  {
    "start": "00:07:52.720000",
    "end": "00:08:23.080000",
    "text": "the filters are simpler in the first layers and become more and more complicated in the subsequent kind of layers to match the sort of nonv visual intuitively visual complexities that we have seen in the output activation maps or feature maps that we have seen earlier"
  }
]