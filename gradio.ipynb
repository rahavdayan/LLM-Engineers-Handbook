{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 2156, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1940, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/components/chatbot.py\", line 627, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "  File \"/home/rahav/.cache/pypoetry/virtualenvs/llm-engineering-sRZUHTNH-py3.11/lib/python3.11/site-packages/gradio/components/chatbot.py\", line 426, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# The chunk retrieval function\n",
    "def grab_relevant_chunks(\n",
    "    question,\n",
    "    top_k=1,\n",
    "    collection_name=\"subtitle_chunks\",\n",
    "    qdrant_host=\"localhost\",\n",
    "    qdrant_port=6333\n",
    "):\n",
    "    # Load the same embedding model used during indexing\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    query_embedding = embedding_model.encode(question)\n",
    "\n",
    "    # Connect to Qdrant\n",
    "    client = QdrantClient(host=qdrant_host, port=qdrant_port)\n",
    "\n",
    "    # Perform similarity search\n",
    "    search_results = client.search(collection_name=collection_name, query_vector=query_embedding, limit=top_k)\n",
    "\n",
    "    # Extract metadata/payloads\n",
    "    top_metadata = [hit.payload for hit in search_results]\n",
    "\n",
    "    return top_metadata\n",
    "\n",
    "# Prompt Generation and Response\n",
    "def generate_response(question):\n",
    "    chunks = grab_relevant_chunks(question)\n",
    "    llm_prompt = f\"\"\"I have a list of video segments that were retrieved in response to the user question: \"{question}\"\n",
    "    Each segment includes a start and end timestamp, the video number it belongs to, and a short transcript (text).\n",
    "    Please do the following for each segment:\n",
    "\n",
    "    1. Give me the start and end timestamps and the video the segment came from.\n",
    "    2. Summarize the main idea or key point explained in the segment in one sentence.\n",
    "    3. Highlight any explanation, definition, or key information related to the user's question.\n",
    "    4. If relevant, rephrase technical descriptions into simpler or more understandable terms.\n",
    "\n",
    "    Here is the list of segments:\n",
    "    ```json\n",
    "    {json.dumps(chunks)}\n",
    "    ```\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages = [{'role': 'user', 'content': llm_prompt}]\n",
    "    )\n",
    "\n",
    "    response_content = response['message']['content']\n",
    "    return response_content\n",
    "\n",
    "# Gradio Interface\n",
    "def chatbot_interface(user_input):\n",
    "    return generate_response(user_input)\n",
    "\n",
    "# Create Gradio chatbot interface\n",
    "chatbot = gr.Chatbot()\n",
    "\n",
    "# Launch the app with Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\"),\n",
    "    outputs=chatbot,\n",
    "    live=True,\n",
    "    title=\"Video-Based Q&A Bot\",\n",
    "    description=\"Ask any question about video content and get answers based on the video segments.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering-sRZUHTNH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
